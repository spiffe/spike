<?xml version="1.0" encoding="utf-8" ?>
<!--
#    \\ SPIKE: Secure your secrets with SPIFFE. â€” https://spike.ist/
#  \\\\\ Copyright 2024-present SPIKE contributors.
# \\\\\\\ SPDX-License-Identifier: Apache-2.0
-->
<!--
  ABOUT JIRA.XML

  JIRA.XML serves as a sandbox for capturing ideas and drafting issue templates.
  It is a tongue-in-cheek jab at how inefficient Jira is at managing tasks and
  how, sometimes, simple tools (like a shared, version-controlled, free-form
  text file) can make wonders because they are easy to use without any red tape
  around them.

  JIRA.xml provides a free-form space where we can â€œthink out loudâ€ and sketch
  potential issues before deciding which ones to formally create in GitHub. By
  working here first, we keep the active issue tracker focused and avoid
  cluttering it with early-stage or exploratory thoughts.
 -->
<stuff>
  <high-level-plan>
    <issue order="3">Secure Secrets Management Web UX</issue>
    <issue order="4">Zero Turtle Pi Grid</issue>
    <watch-list>
      * Outstanding PRs on SDK and SPIKE.
      * cipher encrypt decrypt: https://github.com/spiffe/spike/pull/212
      * encrypt policy info: https://github.com/spiffe/spike/pull/223
      * ConfigMap: https://github.com/spiffe/spike/pull/228
      * status: https://github.com/spiffe/spike/pull/222
      * cipher validation to start.sh: https://github.com/spiffe/spike/issues/242
      --
      * key rotation: https://github.com/spiffe/spike/issues/229
      * secure UI: https://github.com/spiffe/spike/issues/230
    </watch-list>
  </high-level-plan>
  <task-group for="v0.6.1" theme="doc update and stabilization">
    <issue>
      change the image on the landing page; the CLI commands etc has changed
      and the image does not reflect the recent cli.
    </issue>
    <issue>
      go through internal files, some of them are generic enough to graduate
      to the SDK.
    </issue>
    <issue>
      start.sh should test recovery and restore
      start.sh should test encryption and decryption
    </issue>
    <issue>
      Refactor app/spike/internal/cmd/secret/get.go to extract repeated
      marshaling logic (yaml.Marshal, json.Marshal, json.MarshalIndent) into
      helper functions. The same marshal-and-print pattern is duplicated
      multiple times for different format types.
    </issue>
    <issue>
      format inconsistencies.

      for policies we have json and human
      for secrets we have
      if !slices.Contains([]string{"plain",
      "yaml", "json", "y", "p", "j"}, format) {

      and not all operations honor the format parameter anyway.

      anything that provides an output (except for error messages) should
      honor the format parameter
      human->human-friendly (human, h :: alias: plain, p)
      json->valid json (json, j)
      yaml->valid yaml (yaml, y)
    </issue>
    <issue>
      this is from sdk; it should come from env config
      ReadHeaderTimeout: 10 * time.Second,

    </issue>
    <issue>
      func VerifyShamirReconstruction(secret group.Scalar, shares []shamir.Share) {
      this is generic enough to go to the sdk.
    </issue>
    <issue>
      let AI write some tests to all the things.
    </issue>
    <issue priority="medium" category="testing">
      CLI Command Testing Guidance

      This issue documents strategies for testing SPIKE CLI commands with
      varying levels of complexity.

      == UNIT TESTING (No Mocking Required) ==

      The following helper functions can be tested directly without mocking:

      1. Policy Commands (app/spike/internal/cmd/policy/):
         - filter.go: validUUID() - UUID validation
         - fotmat.go: formatPoliciesOutput(), formatPolicy() - output formatting
         - validation.go: validatePolicySpec() - policy spec validation
         - create.go: readPolicyFromFile(), getPolicyFromFlags() - input parsing

      2. Secret Commands (app/spike/internal/cmd/secret/):
         - validation.go: validSecretPath() - path validation
         - print.go: formatTime(), printSecretResponse() - output formatting

      3. Cipher Commands (app/spike/internal/cmd/cipher/):
         - io.go: openInput(), openOutput() - file I/O handling

      Example test pattern for pure functions:
      ```go
      func TestValidUUID(t *testing.T) {
          tests := []struct {
              name     string
              uuid     string
              expected bool
          }{
              {"valid UUID", "123e4567-e89b-12d3-a456-426614174000", true},
              {"invalid", "not-a-uuid", false},
          }
          for _, tt := range tests {
              t.Run(tt.name, func(t *testing.T) {
                  if validUUID(tt.uuid) != tt.expected {
                      t.Errorf("validUUID(%q) = %v, want %v",
                          tt.uuid, !tt.expected, tt.expected)
                  }
              })
          }
      }
      ```

      == HTTP-LEVEL MOCKING (For API Calls) ==

      To test command Run functions that call the SDK API, use httptest.Server
      to mock SPIKE Nexus responses:

      ```go
      func TestSecretGetCommand_Success(t *testing.T) {
          // Create mock server
          server := httptest.NewTLSServer(http.HandlerFunc(
              func(w http.ResponseWriter, r *http.Request) {
                  // Verify request
                  if r.URL.Path != "/v1/store/secret" {
                      t.Errorf("unexpected path: %s", r.URL.Path)
                  }

                  // Return mock response
                  resp := reqres.SecretReadResponse{
                      Data: map[string]string{"key": "value"},
                  }
                  json.NewEncoder(w).Encode(resp)
              },
          ))
          defer server.Close()

          // Configure SDK to use mock server URL
          // (requires setting SPIKE_NEXUS_API_URL env var or similar)
          t.Setenv("SPIKE_NEXUS_API_URL", server.URL)

          // Execute command and verify output
          // ...
      }
      ```

      Challenges with HTTP mocking:
      - SDK uses mTLS, so mock server needs proper TLS config
      - May need to mock X509Source or bypass SPIFFE validation
      - Consider creating test helpers for common mock scenarios

      == INTEGRATION TESTING (With Real SPIKE Nexus) ==

      For full end-to-end testing, use the existing start.sh to spin up
      a real SPIKE environment. The integration_test.go pattern in the
      policy package shows this approach:

      1. Start SPIKE via start.sh (SPIRE + Nexus + Keeper)
      2. Wait for initialization
      3. Run actual CLI commands
      4. Verify results

      ```go
      // +build integration

      func TestPolicyCreateIntegration(t *testing.T) {
          if os.Getenv("SPIKE_INTEGRATION_TEST") == "" {
              t.Skip("Skipping integration test")
          }

          // Use actual spike CLI binary or SDK
          api, err := spike.New()
          if err != nil {
              t.Fatalf("Failed to create API client: %v", err)
          }
          defer api.Close()

          // Create policy
          err = api.CreatePolicy(data.PolicySpec{
              Name: "test-policy",
              // ...
          })
          if err != nil {
              t.Fatalf("CreatePolicy failed: %v", err)
          }

          // Verify policy exists
          policies, _ := api.ListPolicies("", "")
          // Assert...
      }
      ```

      Run integration tests with:
      ```bash
      # Start SPIKE environment first
      ./hack/start.sh

      # Run integration tests
      SPIKE_INTEGRATION_TEST=1 go test -tags=integration ./...
      ```

      == TESTING log.FatalErr CALLS ==

      Functions that call log.FatalErr can be tested using the panic
      recovery pattern:

      ```go
      func TestAuthenticateForPilot_InvalidID(t *testing.T) {
          // Enable panic mode for FatalErr
          t.Setenv("SPIKE_STACK_TRACES_ON_LOG_FATAL", "true")

          defer func() {
              if r := recover(); r == nil {
                  t.Error("Expected panic for invalid ID")
              }
          }()

          AuthenticateForPilot("invalid-spiffe-id")
          t.Error("Should have panicked")
      }
      ```

      == RECOMMENDED APPROACH ==

      1. Start with unit tests for pure helper functions (high value, low effort)
      2. Add HTTP-level mocks for critical API paths
      3. Use integration tests for end-to-end validation in CI

      Current coverage after unit tests:
      - policy commands: ~48%
      - secret commands: ~10%
      - cipher commands: ~18%

      Target: 60%+ coverage for command packages through unit + mock tests.
    </issue>
    <issue>
      // PermissionExecute grants the ability to execute specified resources.
      // One such resource is encryption and decryption operations that
      // don't necessarily persist anything but execute an internal command.
      const PermissionExecute PolicyPermission = "execute"

      ^ are we using this permission in policies properly?
      ^ is it documented properly?
      ^ are other permissions and where they apply documented?
    </issue>
    <issue>
      env using log causes circular dependencies which we fixed by copying
      a few functions; but instead these env value functions should return
      errors and logging should be based on the returned error values.

      those errors also should be sentinel errors that indicate fatal
      failures.

      func KeepersVal() map[string]string {
      const fName = "KeepersVal"

      p := os.Getenv(NexusKeeperPeers)

      if p == "" {
      log.FatalLn(
      fName,
      "message",
      "SPIKE_NEXUS_KEEPER_PEERS must be configured in the environment",
      )
      }
    </issue>
    <issue>
      SDK should return its unique error instance instead of wrapping errors:

      the SDK should return:
      type SDKError struct {
      Code    ErrorCode  // e.g., "ERR_NOT_READY"
      Message string
      Wrapped error
      }

      func (e *SDKError) Is(target error) bool {
      if t, ok := target.(*SDKError); ok {
      return e.Code == t.Code
      }
      return false
      }

      Then you could:
      if errors.Is(err, &sdk.Error{Code: sdk.ErrNotReady}) {

    </issue>
    <issue>
      func FailIfError[T any](
      and its sisters can go the the SDK.
    </issue>
    <issue>
      validPermsList := "read, write, list, super"
      SDK should define these instead.
    </issue>
    <issue>
      if err != nil || v < 0 || v > 255 {
      ^ version check in spike/internal/cmd/cipher/decrypt.go
      currently we support only a single version `1`.
    </issue>
    <issue>
      these can be made generic and added to the SDK:
      app/nexus/internal/state/base/validation.go
    </issue>
    <issue>
      // It's unlikely to have 1000 SPIKE Keepers across the board.
      // The indexes start from 1 and increase one-by-one by design.
      const maxShardID = 1000
      to env var configuration.
    </issue>
    <issue>
      add some more description etc to SPIKE go SDK github repo.
      also the social media banners on both repos needs an overhaul.
    </issue>
    <issue>
      configure these from env
      // (AES-GCM standard nonce is 12 bytes)
      const expectedNonceSize = 12

      // Limit cipherText size to prevent DoS attacks
      // The maximum possible size is 68,719,476,704
      // The limit comes from GCM's 32-bit counter.
      const maxCiphertextSize = 65536
    </issue>

    <issue>
      internal/net/request.go
      ^ can be SDK candidates.

      func VerifyShamirReconstruction(secret group.Scalar, shares []shamir.Share) {
      can go to sdk.
    </issue>
    <issue>
      lease object; may be used for SPIKE HA setup:
      https://msalinas92.medium.com/deep-dive-into-kubernetes-leases-robust-leader-election-for-daemonsets-with-go-examples-f3b9a8858c49
      if needed.
    </issue>
    <issue>
      audit log separation (ADR-0027)
      also audit thigns to do:
      1. Make audit entries immutable - Have handlers return audit details rather than mutating the entry.
      2. Distinguish audit event types - Add an EventType field (lifecycle vs operation) to differentiate enter/exit from actual operations.
      3. Populate or remove unused fields - Either extract SPIFFE ID/UserID/SessionID everywhere, or remove them until you're ready to implement.
      4. Fix Resource semantics - Resource should be the actual entity (secret path, policy name), not query params. Maybe add a separate QueryParams field if needed.
      5. Consider structured event nesting - Something like:
      type AuditEvent struct {
      TrailID   string
      RequestEvent  AuditEntry  // enter/exit
      Operations []AuditEntry  // what happened inside
      }
      6. Add audit configuration - Sampling rates, verbosity levels, field inclusion/exclusion for different deployment scenarios.
    </issue>
    <issue>
      additioanl memory clearing for the sdk:
      https://github.com/spiffe/spike/issues/243
    </issue>
    <issue>
      VSecM has a missing video:
      vsecm missing video(s)
      https://vsecm.com/documentation/getting-started/overview/
    </issue>
    <issue>
      add oom guard

      package memory

      const CgroupPath = "/sys/fs/cgroup/"
      const MemLimitFile = "memory/memory.limit_in_bytes"
      const MemUsageFile = "memory/memory.usage_in_bytes"
      const MemMaxFile = "memory.max"
      const MemCurrentFile = "memory.current"


      type Watcher struct {
      memMax uint64
      memCurrentPath string
      memThreshold uint8
      interval time.Duration
      ctx context.Context
      cancel context.CancelFunc
      }

      func readUint(path string) (uint64, error) {
      b, err := os.ReadFile(path)
      if err != nil {
      return 0, err
      }
      return strconv.ParseUint(strings.TrimSpace(string(b)), 10, 64)
      }

      func discover(memMaxPath, memCurrentPath string) (string, string, error) {
      if memMaxPath == "" {
      maxPathV1 := filepath.Join(CgroupPath, MemLimitFile)
      maxPathV2 := filepath.Join(CgroupPath, MemMaxFile)

      if _, err := os.Lstat(maxPathV2); err == nil {
      memMaxPath = maxPathV2
      } else if _, err = os.Lstat(maxPathV1); err == nil {
      memMaxPath = maxPathV1
      }
      }
      if memCurrentPath == "" {
      currentPathV1 := filepath.Join(CgroupPath, MemUsageFile)
      currentPathV2 := filepath.Join(CgroupPath, MemCurrentFile)
      if _, err := os.Lstat(currentPathV2); err == nil {
      memCurrentPath = currentPathV2
      } else if _, err = os.Lstat(currentPathV1); err == nil {
      memCurrentPath = currentPathV1
      }
      }

      if memMaxPath == "" && memCurrentPath == "" {
      err
      }
      if memMaxPath == "" {
      err
      }
      if memCurrentPath == "" {
      err
      }

      return memMaxPath, memCurrentPath, nil
      }

      func (w *Watcher) run(ctx context.Context) {
      t := time.NewTicker(w.interval)
      defer t.Stop()

      for {
      select {
      case <-ctx.Done():
      log
      return
      case <-t.C:
      current, err := readUint(w.memCurrentPath)
      if err != nil {
      log
      continue
      }

      currentPercentage := float64(current) / float64(w.memMax) * 100
      if currentPercentage >= float64(w.memThreshold) {
      log
      w.cancel()
      return
      }
      log
      }
      }
      }

      func New(memMaxPath, memCurrentPath string, memThreshold uint8, interval time.Duration) (*Watcher, error) {
      if memThreshold < 1 || memThreshold > 100 {
      return nil, err
      }

      if minInterval := 50 * time.Millisecond; interval < minInterval {
      return nil, err
      }

      memMaxPath, memCurrentPath, err = discover(memMaxPath, memCurrentPath)
      if err != nil {
      return nil, err
      }

      if _, err = os.Lstat(memCurrentPath); err != nil {
      return nil, err
      }

      memMax, err := readUint(memMaxPath)
      if err != nil {
      return nil, err
      }

      return &Watcher{
      memMax: memMax,
      memCurrentPath: memCurrentPath,
      memThreshold: memThreshold,
      interval: interval,
      }, nil
      }

      func (w *Watcher) Watch(ctx context.Context) context.Context {
      sync.Once.Do(func() {
      w.ctx, w.cancel = context.WithCancel(ctx)
      go w.run(ctx)
      })
      return w.ctx
      }
    </issue>
    <issue>
      import "errors"

      // IsOneOf returns true if err is equal to any of the errs.
      func IsOneOf(err error, errs ...error) bool {
      for _, e := range errs {
      if errors.Is(err, e) {
      return true
      }
      }
      return false
      }
w
    </issue>
    <issue>
      review the code related to the `spike cipher` command; refactoring beaver may find some tweaking there.
      also update changelog.
    </issue>
    <issue>
      create a demo video about the new bootstrap flow
      - for bare metal
      - for kubernetes/minikube
    </issue>
    <issue>
      maybe we can add a configurable "bootsrap time out" as an env var later. For now, the bootsrap app will try to
      bootstrap the thing in an exponentially-backing-off loop until it succeeds.
    </issue>
    <issue>
      have an official "press kit" to let people download logos and such.
    </issue>
    <issue>
      the nonce generation functions can go to the SDK too.
    </issue>
    <issue>
      isolate this into a function

      // Security: Use a static byte array and pass it as a pointer to avoid
      // inadvertent pass-by-value copying / memory allocation.
      var rootKey [32]byte
      // Security: Zero-out rootKey after persisted internally.
      defer func() {
      // Note: Each function must zero-out ONLY the items it has created.
      // If it is borrowing an item by reference, it must not zero-out the item
      // and let the owner zero-out the item.
      mem.ClearRawBytes(&rootKey)
      }()

      if _, err := rand.Read(rootKey[:]); err != nil {
      log.Fatal(err.Error())
      }

      state.Initialize(&rootKey)
    </issue>
    <issue>
      // I should be Nexus.
      if !spiffeid.IsNexus(env.TrustRoot(), selfSpiffeid) {
      log.FatalF("Authenticate: SPIFFE ID %s is not valid.\n", selfSpiffeid)
      }

      use the similar trust.Authenticate(...) pattern instead of these if checks.
    </issue>
    <issue>
      func contains(permissions []data.PolicyPermission,
      func hasAllPermissions(
      these can be generic helper functions in the sdk.
    </issue>
    <issue>
      u, err := url.JoinPath(
      keeperAPIRoot, string(apiUrl.KeeperContribute),
      )

      these should be methods of SDK instead.
    </issue>
    <issue>
      type ShardGetResponse struct {
      error field should be optional.
    </issue>
    <issue>
      also, empty id or path should raise an error for policies
      and also for secrets
    </issue>
    <issue>
      func readPolicyFromFile(filePath string) (Spec, error) {
      this function is in the wrong file!
    </issue>
    <issue>
      // TODO: this check should have been within state.CheckAccess
      // maybe we can fork based on spike/system/secrets/encrypt.
      //
      // Lite Workloads are always allowed:
      allowed := false
      if spiffeid.IsLiteWorkload(
      env.TrustRootForLiteWorkload(), peerSPIFFEID.String()) {
      allowed = true
      }
      // If not, do a policy check to determine if the request is allowed:
      if !allowed {
    </issue>
    <issue>
      implement this sometime:
      ci/wip-draft.txt
      and find a place to run it every commit; could be github actions; but also
      can take a long time, so could be a dedicated toy server too.
    </issue>
    <issue>
      func guardDecryptSecretRequest(
      _ reqres.SecretDecryptRequest, w http.ResponseWriter, r *http.Request,
      ) error {
      // TDO: some of these flows can be factored out if we keep the `request`
      // a generic parameter. That will deduplicate some of the validation code.

      peerSPIFFEID, err := spiffe.IdFromRequest(r)
      if err != nil {
      responseBody := net.MarshalBody(reqres.SecretDecryptResponse{
      Err: data.ErrUnauthorized,
      }, w)
      net.Respond(http.StatusUnauthorized, responseBody, w)
      return apiErr.ErrUnauthorized
      }

      :w

    </issue>
    <issue>
      create a banner for SPIKE too; vSecM has fancy banner that show when sharing links on bluesky.

      https://github.com/vmware/secrets-manager/security/dependabot
    </issue>
    <issue>
      check all log.Log()s; log.FatalF() and log.FatalLn()s.
      some of them don't have fName
    </issue>
    <issue>
      // The encryption key must be 16, 24, or 32 bytes in length (for AES-128,
      // AES-192, or AES-256 respectively).

      Nope. we only use 32 bytes.
      fix the docs and also the tests.
    </issue>
    <issue>
      verify that these still work:
      * https://spike.ist/getting-started/local-deployment/
      * https://spike.ist/getting-started/quickstart/
      * https://spike.ist/getting-started/bare-metal/

      Fix the document for chart-related things, if needed.
    </issue>
    <issue>
      fix bare metal instructions.
      a lot of files have new paths now and the instructions will fail.
    </issue>
    <issue>
      type GuardFunc[Req any] func(Req, http.ResponseWriter, *http.Request) error

      // Change function signature [BREAKING]
      func HandleRequest[Req any, Res any](
      requestBody []byte,
      w http.ResponseWriter,
      r *http.Request,
      errorResponseForBadRequest Res,
      guard GuardFunc[Req], // Required parameter
      ) *Req {
      // ... existing logic ...

      if err := guard(request, w, r); err != nil {
      return nil
      }

      return &request
      }
    </issue>
    <issue>
      SPIKE Documentation
      Build the Project
      Make sure you have a running local Docker daemon and execute the following:
      make build-local
      ^
      missing documentation. this directive assumes that we have a local registry
      at port 5000; so we need to start minikube as well.

      will remove spike (feature/k8s)$ eval $(minikube -p minikube docker-env)
      from docs.
    </issue>
    <issue>
      Maybe have an env var for that configuration too.

      Ensure SPIKE Pilot does not indefinitely hang up if SPIRE Nexus is not there
      or there is a SPIFFEID/SVID issue. It should give up after a while and
      print a warning that a connection to the api server could not be established
      in a timely manner.

      We had a timeout configurable somewhere; we can verify that and document
      it in "best practices" section. The timeout was infinite by default,
      I think.
    </issue>
    <issue>
      demo main.go:

      path := "^tenants/demo/db/creds"
      path := "tenants/demo/db/creds"
      the first one creates an error in demo app but the error response is
      not helpful.
    </issue>
    <issue>
      var permissions []data.PolicyPermission
      if permsStr != "" {
      for _, perm := range strings.Split(permsStr, ",") {
      perm = strings.TrimSpace(perm)
      if perm != "" {
      permissions = append(permissions, data.PolicyPermission(perm))
      }
      }
      }
      ^ this needs sanitization in case an invalid permission is passed.
    </issue>
    <issue>
      func DatabaseOperationTimeout() time.Duration {
      ^ this is not used anywhere. find where it should be used and add it.
    </issue>
    <issue>
      add to docs:
      sourcing
      source /home/volkan/WORKSPACE/spike/hack/lib/env.sh
      in your profile file can be helpful for development
      it has predefined environment variables for bare-metal local
      development.
      `make start` already does that for the apps that it launches.
    </issue>
    <issue>
      address TODO:s in the source code.
    </issue>
    <issue>
      missingFlags = append(missingFlags, "name")
      }
      if pathPattern == "" {
      missingFlags = append(missingFlags, "path-pattern")
      }
      if SPIFFEIDPattern == "" {
      missingFlags = append(missingFlags, "spiffeid-pattern")
      }
      if permsStr == "" {
      missingFlags = append(missingFlags, "permissions")

      have these flag names as constants maybe.
    </issue>
    <issue>
      func initializeSqliteBackend(rootKey *[32]byte) backend.Backend {
      panic if rootkey is nil or empty.
    </issue>
    <issue>
      create a demo video for the new bootstrapping feature.
    </issue>
    <issue>
      create a video about how to develop SPIKE on WSL.
    </issue>
    <issue>
      remove path normalization tests, since we are not normalizing paths anymore.

      //{
      //	name:         "flags_with_multiple_trailing_slashes",
      //	inputName:    "multi-slash-policy",
      //	inputSpiffed: "^spiffe://example\\.org/test/.*$",
      //	inputPath:    "^secrets/test///$",
      //	inputPerms:   "read",
      //	expectedPath: "^secrets/test$",
      //	wantErr:      false,
      //},
    </issue>
    <issue>
      Since the architecture has changed a bit, a re-introduction to SPIKE
      demo video is due. But before that, update all the diagrams that need
      updating, since you can use them while doing the into architecture
      walkthrough too.
    </issue>
    <issue>
      update documentation to provide details about SPIKE Bootstrap
      Also, remove/edit part that are not relevant anymore due to the
      introduction of SPIKE Bootstrap.
      also, check the docs to see if there is any diagram that need an
      update.

      The flows here have changed; need new diagrams and updated docs:
      https://spike.ist/architecture/system-overview/

      https://spike.ist/getting-started/quickstart/
      needs update once helm charts is updated.
    </issue>
    <issue>
      demo: new architecture overview.
    </issue>
    <issue>
      there is a bug filed for this; if nobody fixes by then, close the bug too
      after you fix this.

      reading an unknown secret gives an error instead of "not found"

      âžœ  spike git:(feature/bootstrap-job) âœ— spike secret get tenants/demo/db
      Error: failure reading secret: post: Problem connecting to peer
      Usage:
      spike secret get <path> [key] [flags]

      Flags:
      -f, --format string   Format to use. Valid options: plain, p, yaml, y, json, j (default "plain")
      -h, --help            help for get
      -v, --version int     Specific version to retrieve

      failure reading secret: post: Problem connecting to peer
    </issue>
    <issue>
      when entries are not registered to the SPIRE Server and the
      operator tries to use SPIKE, the error messages can be more
      explanatory.
    </issue>
    <issue>
      when SPIKE is active via `make start`, `make test` fails.
    </issue>
    <issue>
      add `-trimpath` and `-a` to go build.
    </issue>
>
    <issue>
      during bootstrap retry; if a single keeper fails, it will retry it
      indefinitely without giving chance to other keepers.
      a better solution is needed for that.
    </issue>
    <issue>
      test what happens if there are no keepers available and initialized
      but bootstrap still tries to bootstrap
    </issue>
    <issue>
      verify `make start` works in in-memory mode.
      it likely tries to trigger bootstrap and fail
      if it fails, give a proper warning in bootstrap instead of random panics.
    </issue>
    <issue>
      if there are no binaries, `make start` errs out; it should not, since it
      builds the binaries before anything else anyway.
    </issue>
    <issue>
      spike bootstrap and other container images don't have any descriptions

      No description provided
      This package version was published 7 minutes ago.

      To provide a description, add the following line to your Dockerfile:

      LABEL org.opencontainers.image.description DESCRIPTION
      For multi-arch images, set a value for the org.opencontainers.image.description key in the annotations field of the manifest:

      "annotations": { "org.opencontainers.image.description": "DESCRIPTION" }
      https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#labelling-container-images
    </issue>
    <issue>
      start.sh should test encryption as a service too; both stream mode and rest mode.
    </issue>
    <issue>
      these serialization and deserialization functions can be
      extracted. --- They can even be part of the Go SDK.

      It can error, or swallow unknown permissions.

      // Deserialize permissions from comma-separated string
      if permissionsStr != "" {
      permissionStrs := strings.Split(permissionsStr, ",")
      policy.Permissions = make([]data.PolicyPermission, len(permissionStrs))
      for i, permStr := range permissionStrs {
      policy.Permissions[i] = data.PolicyPermission(strings.TrimSpace(permStr))
      }
      }
    </issue>
    <issue>
      // this check should have been within state.CheckAccess
      // maybe we can fork based on spike/system/secrets/encrypt.
      //
      // Lite Workloads are always allowed:
      allowed := false
      if spiffeid.IsLiteWorkload(
      env.TrustRootForLiteWorkload(), peerSPIFFEID.String()) {
      allowed = true
      }
    </issue>
    <issue>
      move generated binaries to `./bin` folder.
      stream logs to `./logs` folder.
    </issue>
    <issue>
      these may go to the sdk

      func GenerateCustomNonce(s *DataStore) ([]byte, error) {
      nonce := make([]byte, s.Cipher.NonceSize())
      if _, err := io.ReadFull(rand.Reader, nonce); err != nil {
      return nil, err
      }
      return nonce, nil
      }

      func EncryptWithCustomNonce(s *DataStore, nonce []byte, data []byte) ([]byte, error) {
      if len(nonce) != s.Cipher.NonceSize() {
      return nil, fmt.Errorf("invalid nonce size: got %d, want %d", len(nonce), s.Cipher.NonceSize())
      }
      ciphertext := s.Cipher.Seal(nil, nonce, data, nil)
      return ciphertext, nil
      }
    </issue>
    <issue>
      kind of thinking of a new mode for spike.... something like calling it spike lite or something.
      Where we basically turn off the secrets and policy api, and add an endpoint for encrypt / decrypt.

      2:51
      we could still do a full s3 backend too, but for someone just wanting to handle things themselves, but have the
      power of nexus/keepers for encrypting/decrypting things just stored somewhere, it might be a nice feature.
      2:52
      what do you thnk?

      Volkan Ozcelik
      5:09 PM
      could be useful.

      SPIKE Lite:
      # get pilot's pem and key to test things out.
      # this can be part of documentation too. to test the API directly, 1. extract the pem and key, and then do regular
      curl.
      curl -s -X POST --header "Content-Type:application/octet-stream" --data-binary "This is a test encryption"
      https://spire-spike-nexus/v1/encrypt -k --cert /tmp/pem/svid.0.pem --key /tmp/pem/svid.0.key -o encrypted
      curl -s -X POST --header "Content-Type:application/octet-stream" --data-binary @encrypted
      https://spire-spike-nexus/v1/decrypt -k --cert /tmp/pem/svid.0.pem --key /tmp/pem/svid.0.key -o decrypted
      cat decrypted; echo
    </issue>
    <issue severity="important" urgency="moderate">
      // TDO: check all database operations (secrets, policies, metadata)
      and
      // ensure that they are retried with exponential backoff.

      add retries to everything under:
      app/nexus/internal/state/persist
      ^ they all talk to db; and sqlite can temporarily lock for
      a variety of reasons.
    </issue>
    <issue>
      Try SPIKE on a Mac (and create a video)
    </issue>
    <issue>
      Try SPIKE on an x-86 Linux (and create a video)
    </issue>
    <issue>
      Check what else needed (aside from enabling fips-algorithms) to
      be fips-compatible.
    </issue>
    <issue>
      also check out: https://developer.hashicorp.com/vault/docs/concepts/policies
      to see if we can amend any updates to the policy rules
      (one such update, for example, is limiting what kind of attributes are
      allowed, but we should discuss whether that much granularity is worth the
      hassle)
    </issue>
    <issue>
      ADR:
      * Added the ability to optionally skip database schema creation during SPIKE
      initialization. This can be useful if the operator does not want to give
      db schema modification privileges to SPIKE to adhere to the principle of
      least privilege. The default behavior is to allow automatic schema creation.
      Since SPIKE is assumed to own its backing store, limiting its access
      does not provide a significant security benefit. Letting SPIKE manage
      its own database schema provides operational convenience.
    </issue>
    <issue>
      ErrPolicyExists = errors.New("policy already exists")
      ^ this error is never used; check why.
    </issue>
    <issue>
      before trying to get source...
      remove this log.
    </issue>
    <issue>
      // TDO: Yes memory is the source of truth; but at least
      // attempt some exponential retries before giving up.
      if err := be.StoreSecret(ctx, path, *secret); err != nil {
      // Log error but continue - memory is the source of truth
      log.Log().Warn(fName,
      "message", "Failed to cache secret",
      "path", path,
      "err", err.Error(),
      )
      }

      SQLLite can error out if there is a blocked transaction or
      a integrity issue, which a retry can fix it.
    </issue>
    <issue>
      update the guides: PSP is not a thing anymore; better update it
      to Pod Security Standards.
    </issue>
    <issue>
      CLI debug logging to file.

      Currently, the spike CLI (spike policy, spike secret, etc.) does not
      have any structured logging for debugging purposes. When errors occur,
      only user-friendly messages are shown.

      Consider adding an optional file-based logging mechanism for the CLI
      that can be enabled via an environment variable (e.g., SPIKE_CLI_LOG)
      or a flag (e.g., --debug-log=/path/to/file). This would help with
      troubleshooting without cluttering the terminal output.

      The logs should be structured (JSON) and include error codes, timestamps,
      and context information.
    </issue>

  </task-group>
  <task-group for="v0.6.3">
    <issue priority="high" category="bug" repo="spike-sdk-go">
      SDK Recover/Restore functions use wrong SPIFFE ID check

      The SDK's `Recover` function in
      `spike-sdk-go/api/internal/impl/operator/recover.go` checks for generic
      pilot identity instead of the recover role.

      == Bug Location ==

      recover.go line 100:
        if !spiffeid.IsPilot(selfSPIFFEID) {

      == Expected ==

        if !spiffeid.IsPilotRecover(selfSPIFFEID) {

      == Impact ==

      The `spike operator recover` command fails with:
        "recovery can only be performed from SPIKE Pilot"

      Even when the caller has the correct recover role SPIFFE ID:
        spiffe://spike.ist/spike/pilot/role/recover

      The SDK is checking for the generic pilot ID:
        spiffe://spike.ist/spike/pilot

      == Files ==
      - spike-sdk-go/api/internal/impl/operator/recover.go:100

      == Notes ==
      The same bug likely exists in the `Restore` function. Check
      `spike-sdk-go/api/internal/impl/operator/restore.go` for similar issue.
    </issue>
    <issue priority="medium" category="bug">
      spike cipher JSON mode: encrypt discards version and nonce

      The JSON mode encrypt/decrypt cannot round-trip because critical data
      is lost during encryption output.

      == Problem ==

      When using JSON mode encryption:
        spike cipher encrypt --plaintext "$BASE64_PLAINTEXT"

      The API returns a CipherEncryptResponse with:
        - Version (byte)
        - Nonce ([]byte)
        - Ciphertext ([]byte)

      But the CLI (encrypt_impl.go:encryptJSON) only writes the Ciphertext
      to output, discarding Version and Nonce.

      To decrypt in JSON mode, the user needs:
        spike cipher decrypt --version V --nonce NONCE_B64 --ciphertext CT_B64

      Since version and nonce are never output, JSON encrypt â†’ JSON decrypt
      cannot work.

      == Streaming Mode Works ==

      Streaming mode works correctly because the server packages
      version+nonce+ciphertext into a single binary blob that round-trips.

      == Proposed Fix ==

      Option 1: Output structured JSON from encrypt:
        {
          "version": 1,
          "nonce": "base64...",
          "ciphertext": "base64..."
        }

      Option 2: Output the same binary format as streaming mode.

      Option 3: Remove JSON mode flags if streaming covers all use cases.

      == Files ==
      - app/spike/internal/cmd/cipher/encrypt_impl.go (encryptJSON function)
      - app/spike/internal/cmd/cipher/decrypt_impl.go (decryptJSON function)
      - SDK: api/entity/v1/reqres/cipher.go (CipherEncryptResponse struct)
    </issue>
    <issue waitingFor="upstreamHelmCharts">
      after helm-charts changes are merged, verify that the quickstart
      guide still works.
    </issue>
    <issue>
      // TO: both verification and retry is done forever; we need an upper limit
      // for these.
      // we can have a goroutine/channel that panic if this entire main thread
      // does not complete within a given timeframe.
    </issue>
    <issue>
      // TDO: both verification and retry is done forever; we need an upper limit
      // for these.
      // we can have a goroutine/channel that panic if this entire main thread
      // does not complete within a given timeframe.
    </issue>
    <issue>
      SDK exponential retry unit tests are still flaky.
      maybe create them from scratch.
    </issue>
    <issue>
      create a list for what needs for 1.0 version; i.e. to move SPIKE
      out of alpha and make it "ready for production use with a hint of caution"

      SPIFFE Org already has standard requirements and a process for that.
      Check it out.
    </issue>
    <issue>
      Generate coverage reports for the SDK too.
    </issue>
    <issue waitingFor="upstreamHelmCharts">
      Go over entire documentation: There are places to update since
      we have helm charts updates.
    </issue>
    <issue status="assigned">
      make tests concurrent again.
    </issue>
    <issue>
      const KeeperKeep APIURL = "/v1/store/keep"
      const KeeperContribute APIURL = "/v1/store/contribute"
      const KeeperShard APIURL = "/v1/store/shard"

      ^ these are unused in the SDK, which may hint that whatever uses them
      here may need to be SDK methods maybe.
    </issue>
    <issue>
      bootstrap verification:
      encrypt a known txt and send it to nexus,
      if it can decrypt, bootstrap is complete,
      if not, fail the job and log an error both on bootstrap and also nexus.
    </issue>
    <issue>
      for spike consider some of these if not there yet already
      https://fluxcd.io/flux/security/
    </issue>
    <issue>
      Sign generated binaries:
      #!/usr/bin/env bash
      set -euo pipefail

      ART_DIR="${1:-dist}"
      MODE="${MODE:-kms}"  # kms|keyless|file
      KEY="${KEY:-awskms://arn:aws:kms:us-west-2:123456789012:key/XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX}"
      PUB="${PUB:-spike-cosign.pub}"

      cd "$ART_DIR"

      # 1) checksums
      sha256sum * > SHA256SUMS

      signfile () {
      local f="$1"
      case "$MODE" in
      kms)
      cosign sign-blob --key "$KEY" --output-signature "$f.sig" "$f"
      ;;
      file)
      cosign sign-blob --key cosign.key --output-signature "$f.sig" "$f"
      ;;
      keyless)
      COSIGNEXPERIMENTAL=1 cosign sign-blob --yes \
      --output-signature "$f.sig" \
      --output-certificate "$f.pem" \
      --bundle "$f.bundle" \
      "$f"
      ;;
      *)
      echo "Unknown MODE=$MODE" >&2; exit 1
      ;;
      esac
      }

      # 2) sign all artifacts + checksum file
      for f in *; do
      [[ "$f" =~ .(sig|pem|bundle)$ ]] && continue
      sign_file "$f"
      done

      # 3) export public key if using KMS
      if [[ "$MODE" == "kms" ]]; then
      cosign public-key --key "$KEY" > "../$PUB"
      fi

      echo "Signed. Publish signatures + $PUB (if present)."
    </issue>
    <issue>
      better to have different fns per content type instead

      this is from go sdk

      // Encrypt encrypts either via streaming or JSON based on mode.
      // Stream mode: send r as body with contentType. Returns ciphertext bytes.
      // JSON mode: send plaintext + algorithm; returns ciphertext bytes.
      func Encrypt(
      source *workloadapi.X509Source, mode Mode, r io.Reader,
      contentType string, plaintext []byte, algorithm string,
      allow predicate.Predicate,
      ) ([]byte, error) {
    </issue>
    <issue>
      func (a *API) CipherDecryptJSON(
      version byte, nonce, ciphertext []byte, algorithm string,
      ) ([]byte, error) {

      better to define an algorithm type so that users can pick from
      supported algorithms.
    </issue>
    <issue>
      SPIKE Bootstrap tries to reach keepers forever, but it should have a max
      timeout (like, say, 20mins), after which it gives up and crashes.
      -- configurable.
    </issue>
    <issue>
      bootstrap: when bootstrap is done encrypt a text and let nexus decrypt it
      to ensure that the bootstrapping is done and the root key works.
    </issue>
    <issue>
      consider using modernize:
      https://pkg.go.dev/golang.org/x/tools/gopls/internal/analysis/modernize
    </issue>
    <issue>
      introduce SPIKE to VSecM
      https://github.com/vmware/secrets-manager/issues/1275
    </issue>
  </task-group>
  <task-group for="v0.6.2">
    <issue>
      demo: encryption as a service
    </issue>
    <issue>
      You have OIDC-related drafts; polish them and move them somewhere visible
      as an actionable roadmap.
    </issue>
    <issue>
      for operator actions ensure that `spike/system/ops` path is enforced
      (similar to how acl is enforced above.

      create a workload that can create policies
      create another workload that can backup the root key
      to demo these features.
    </issue>
    <issue>
      Need to think about this; I'm not sure we need a UI at all.
      I am getting more fond of Claude Code model of triggering a web-device-PKCE
      cfg flow and then doing everything through the CLI.

      For VSecM: create a UI that directly talks to SPIKE; depracate all VSecM
      secret model. also delete most of the folders. start with a clean slate
    </issue>
    <issue>
      some test helpers we use can move to the SDK.
    </issue>
    <issue>
      check how the progress of this is going:
      https://github.com/spiffe/spike/issues/238
    </issue>
    <issue>
      // this pattern is repeated a lot; move to a helper function.
      _ = os.Setenv("SPIKE_NEXUS_BACKEND_STORE", "memory")
      defer func() {
      if original != "" {
      _ = os.Setenv("SPIKE_NEXUS_BACKEND_STORE", original)
      } else {
      _ = os.Unsetenv("SPIKE_NEXUS_BACKEND_STORE")
      }
      }()
    </issue>
  </task-group>
  <task-group for="later">
    <issue>
      YAML-based secret creation from SPIKE CLI.

      Follow up: for k8s clusters, a custom resource that manages these secrets securely
      (i.e., values will be encrypted, and can only be decrypted through SPIKE Pilot; we can create
      a cli helper for that (`kubectl get --raw /apis/example.com/v1/namespaces/default/spikesecrets/demo-db/reveal`)
      (as a custom resource)
      or
      a kubectl plugin
      `curl -H "Authorization: Bearer $(kubectl config view --raw -o jsonpath='{.users[0].user.token}')" \
      "https://spike-controller.default.svc/reveal?name=$1"`
      or a CSI Driver that mounts secrets to a volume.
      or something like that:

      # Direct API call for cluster admins
      kubectl get --raw /apis/spike.io/v1/namespaces/default/spikesecrets/demo-db/reveal
      # Or with a simple kubectl plugin wrapper
      kubectl spike reveal demo-db
      # Or
      kubectl spike reveal demo-db -o yaml

      cid:6062f33b-4a94-4b0f-a306-c13c4dd3d777
    </issue>
    <issue>
      check all test helpers. some of them can go to the SDK instead.
    </issue>
    <issue>
      // to SDK
      //// Helper function to create a random test key
      //func createRandomTestKey(t *testing.T) *[crypto.AES256KeySize]byte {
      // key := &[crypto.AES256KeySize]byte{}
      // if _, err := rand.Read(key[:]); err != nil {
      // t.Fatalf("Failed to generate random test key: %v", err)
      // }
      // return key
      //}
    </issue>
    <issue>
      Implement `spike secret purge` command to remove empty secrets from
      storage.

      After all versions of a secret are deleted, the secret metadata
      remains in the backend storage indefinitely. This causes unbounded
      storage growth.

      The purge command should:
      - Scan all secrets in the backend
      - Identify secrets where CurrentVersion == 0 (all versions deleted)
      - Use the backend's Destroy() method to permanently remove them
      - Report how many secrets were purged

      Usage:
        spike secret purge              # Interactive confirmation
        spike secret purge --force      # Skip confirmation
        spike secret purge --dry-run    # Show what would be purged

      Implementation notes:
      - spike-sdk-go now has Value.IsEmpty() and KV.Destroy() methods
      - SPIKE Nexus backend needs corresponding Destroy() implementation
      - Consider adding --older-than flag for time-based purging
    </issue>
  </task-group>
  <task-group for="SOC2-compliance">
    <issue>
      Update:
      This needs to be re-evaluated. As long as we make SPIKE log formatting
      SOC-compliant; and streamable to a log aggregator, we don't necesarrily
      need to implement an opinionated logging infra, as users will likely
      be using their own secure log aggregation solutions anyway.

      It is possible to have a reference infrastructure somewhere, but I think
      it's out of SPIKE (and also VSecM's) scope.

      ---

      SOC-2 Compliance Checklist for SPIKE Logging System

      ## Overall Plan

      * Make SPIKE Audit logs streamable, structured, and have mandatory SOC-2
        fields and actions.
      * Position VSecM as a centralized log manager that does the heavy-lifting.

      ## Overview

      This checklist helps ensure the SPIKE's and VSecM's logging infrastructure
      meets SOC-2 compliance requirements under the Trust Services Criteria,
      with a focus on Security (mandatory), Availability, Processing Integrity,
      Confidentiality, and Privacy.

      ## 1. Logging Infrastructure Setup

      ### Core Logging Requirements

      - [ ] **Centralized Log Management** (VSecM)
      - Implement a centralized log repository for all SPIKE components
        (Nexus, Keeper, Pilot, Bootstrap)
      - Use structured logging format (JSON preferred)
      - Ensure logs from all environments are aggregated

      - [ ] **Log Collection Scope**
      - Application logs (errors, warnings, info, debug)
      - Authentication and authorization events
      - System access logs (successful and failed attempts)
      - API request/response logs
      - Security events and anomalies
      - Configuration changes
      - Administrative actions
      - Data access and modification events
      - Network traffic logs

      - [ ] **Real-time Monitoring**
      - Implement continuous, real-time log collection
      - Set up automated log analysis
      - (potential future item) Configure SIEM integration.

      ## 2. Log Content Requirements

      ### Mandatory Fields for Each Log Entry
      - [ ] **Timestamp** (ISO 8601 format with timezone)
      - [ ] **Event Type/Category** (authentication, authorization, error, etc.)
      - [ ] **User Identity** (user ID, service account, or system)
      - [ ] **Source IP Address**
      - [ ] **Component/Service Name** (Nexus, Keeper, Pilot)
      - [ ] **Action Performed**
      - [ ] **Result** (success/failure)
      - [ ] **Session ID** (for correlation)
      - [ ] **Unique Log ID**

      ### Security-Specific Events to Log

      - [ ] **Authentication Events**
      - Login attempts (successful/failed)
      - Logout events
      - Password changes
      - MFA events
      - Session timeouts

      - [ ] **Authorization Events**
      - Access grants/denials
      - Privilege escalations
      - Role changes
      - Permission modifications

      - [ ] **Secret Management Events**
      - Secret creation/deletion
      - Secret access requests
      - Secret modifications
      - Key rotation events (feat: implement automatic root key rotation)
      - Root key management operations

      - [ ] **System Changes**
      - Configuration modifications
      - Software updates/patches
      - Security policy changes
      - Certificate updates

      ## 3. Log Protection and Integrity

      ### Security Controls
      - [ ] **Encryption**
      - Encrypt logs in transit (TLS 1.2+)
      - Encrypt logs at rest (AES-256 or stronger)

      - [ ] **Access Controls**
      - Implement role-based access control (RBAC) for log access
      - Principle of least privilege for log viewers
      - Separate duties for log administration
      - Multi-factor authentication for log access

      - [ ] **Integrity Protection**
      - Implement log tampering detection
      - Use write-once storage where possible
      - Generate checksums/hashes for log files
      - Implement log signing for critical events

      - [ ] **Segregation**
      - Separate production logs from development/testing
      - Isolate sensitive data logs with additional controls

      ## 4. Log Retention and Availability

      ### Retention Policies

      - [ ] **Define Retention Periods** (all configurable)
      - Security event logs: minimum 12 months
      - Access logs: minimum 90 days
      - System logs: minimum 6 months
      - Audit logs: minimum 7 years (or per regulatory requirements)

      - [ ] **Backup and Recovery**
      - Regular automated backups of log data
      - Test restore procedures quarterly
      - Off-site/cloud backup storage
      - Document recovery time objectives (RTO)

      - [ ] **Availability Requirements**
      - Ensure 99.9% availability of logging system
      - Implement redundancy for log collectors
      - Set up failover mechanisms
      - Monitor logging system health


      ## 5. Monitoring and Alerting

      ### Alert Configuration

      - [ ] **Security Alerts**
      - Multiple failed authentication attempts
      - Unauthorized access attempts
      - Privilege escalation events
      - Configuration changes
      - Suspicious API usage patterns
      - Log system failures

      - [ ] **Threshold Settings**
      - Define baselines for normal activity
      - Set anomaly detection thresholds
      - Configure rate-limiting alerts
      - Establish escalation procedures

      - [ ] **Response Procedures**
      - Document incident response procedures
      - Define alert priority levels
      - Establish notification chains
      - Set response time SLAs

      ## 6. Compliance and Audit Support

      ### Documentation

      - [ ] **Logging Policy**
      - Create a formal logging and monitoring policy
      - Define what must be logged
      - Specify retention requirements
      - Document access procedures

      - [ ] **Procedures Documentation**
      - Log review procedures
      - Incident investigation procedures
      - Log export/reporting procedures
      - Evidence collection procedures

      ### Audit Trail Features

      - [ ] **Evidence Collection**
      - Automated evidence gathering capabilities
      - Log export in standard formats (CSV, JSON)
      - Chain of custody documentation
      - Audit report generation

      - [ ] **Compliance Mapping**
      - Map log events to SOC-2 controls
      - Create compliance dashboards
      - Generate compliance reports
      - Track control effectiveness

      ## 7. Privacy and Data Protection

      ### PII Handling

      - [ ] **Data Minimization**
      - Avoid logging sensitive personal data
      - Mask/redact PII in logs (SSN, credit cards, etc.)
      - Implement field-level encryption for sensitive data

      - [ ] **Privacy Controls**
      - Implement data classification
      - Tag logs containing personal data
      - Apply appropriate retention policies
      - Enable right-to-erasure capabilities

      ## 8. Implementation Checklist

      ### Technical Implementation
      - [ ] **Logging Framework**
      - Choose an appropriate logging library (e.g., structured logging)
      - Implement correlation IDs across services
      - Add context to all log entries
      - Standardize log formats across components

      - [ ] **Infrastructure Setup**
      - Deploy log aggregation system (preferably open source with a non-evil license)
      - Configure log forwarders/agents
      - Set up log storage infrastructure
      - Implement log rotation policies

      - [ ] **Integration Points**
      - SPIFFE/SPIRE integration for identity
      - mTLS logging for Pilot API calls
      - Keeper redundancy event logging
      - Nexus encryption/decryption audit logs

      ## 9. Testing and Validation

      ### Testing Requirements
      - [ ] **Log Generation Testing**
      - Verify all required events are logged
      - Test log format consistency
      - Validate timestamp accuracy
      - Check correlation ID propagation

      - [ ] **Security Testing**
      - Test access controls
      - Verify encryption implementation
      - Attempt log tampering (should fail)
      - Test log injection prevention

      - [ ] **Performance Testing**
      - Measure logging overhead
      - Test high-volume scenarios
      - Verify no log loss under load
      - Monitor storage growth rates

      ## 10. Ongoing Maintenance

      ### Regular Activities

      - [ ] **Monthly Tasks**
      - Review log storage capacity
      - Analyze security alerts
      - Update alert thresholds
      - Review access logs

      - [ ] **Quarterly Tasks**
      - Test backup/restore procedures
      - Review and update logging policy
      - Conduct log retention cleanup
      - Perform access reviews

      - [ ] **Annual Tasks**
      - Complete logging policy review
      - Update risk assessment
      - Conduct penetration testing
      - Review with external auditor

      ## 11. Metrics and KPIs

      ### Monitoring Metrics
      - [ ] **Operational Metrics**
      - Log ingestion rate
      - Storage utilization
      - Query response times
      - System availability percentage

      - [ ] **Security Metrics**
      - Failed authentication attempts
      - Unauthorized access attempts
      - Time to detect incidents
      - Mean time to respond (MTTR)

      - [ ] **Compliance Metrics**
      - Percentage of systems with logging enabled
      - Log retention compliance rate
      - Audit finding closure rate
      - Evidence collection time

      ## 12. Tools and Technologies to Consider

      ### Logging Solutions

      - **Open Source Options:**
      - Fluentd + Prometheus + Grafana
      - Graylog
      - Apache Kafka for log streaming

      ### SIEM Integration

      - Configure integration with Security Information and Event Management systems
      - Enable correlation rules
      - Set up automated threat detection

      ## Priority Actions for SPIKE

      Given SPIKE's architecture as a secrets management system using SPIFFE:

      1. **Immediate Priorities:**
      - Implement comprehensive audit logging for all secret operations
      - Add mTLS certificate validation logging
      - Log all SPIFFE identity verification events
      - Capture root key access and management events

      2. **Critical Security Events:**
      - Log every secret encryption/decryption operation
      - Track Keeper failover events
      - Monitor Pilot CLI authentication and commands
      - Record all administrative access to Nexus

      3. **Integration Requirements:**
      - Ensure SPIRE integration includes workload attestation logs
      - Log SVID issuance and rotation
      - Capture trust bundle updates
      - Monitor federation events if applicable


      ## Notes

      - This checklist should be reviewed and updated quarterly
      - SOC-2 Type II requires demonstrating control effectiveness
        over 3-12 months
    </issue>
  </task-group>
  <task-group for=">v0.5.0;fix-tests">
    <issue>
      fix:
      func TestInitialize_MemoryBackend_ValidKey(t *testing.T) {
    </issue>
    <issue>
      Fix LogFatalXXX-related tests.
      * func TestPostHTTPInteraction(t *testing.T) {

      Fix:
      //func TestNew_CipherCreationFailure(t *testing.T) {
    </issue>
    <issue>
      fix func TestInitializeBackend_UnknownType_DefaultsToMemory(t *testing.T) {
      it is a logic error, so better to fix this release rather than waiting for
      the next one.

      same here:
      func TestInitializeBackend_NoEnvironmentVariable_DefaultsToMemory(t *testing.T) {
    </issue>
    <issue>
      Fix:
      func TestNew_InvalidKey(t *testing.T) {
    </issue>
    <issue>
      Fix:
      func TestUnmarshalShardResponse_InvalidInput(t *testing.T) {

      Fix:
      func TestURLJoinPath(t *testing.T) {

      Fix:
      func TestURLJoinPathForKeepers(t *testing.T) {
    </issue>
    <issue>
      Make this testable:

      fmt.Println("")
      fmt.Println("Usage: bootstrap -init")
      fmt.Println("")
      os.Exit(1) // define a global osExit function or maybe in SDK, to be able to test stuff.
    </issue>
    <issue>
      func TestKeeperIDConversion(t *testing.T) {
      keeper ids need to be stricter. add validation logic to the code.
    </issue>
    <issue>
      func TestShardURL_InvalidInput(t *testing.T) {
      Keeper API root shall not be empty.
    </issue>
  </task-group>

  <immediate-backlog>
    <issue>
      make the CI folder work again.
      there is a wip-draft.txt there to think about.
      we can run the tests in gh actions inside a container probably.
    </issue>
    <issue>
      update SPIKE pilot to be able to delete multiple policies or secrets
    </issue>
    <issue>
      go through all files and create tests for them.
      create a dedicated PR just for tests.
    </issue>
    <issue>
      This how policy list works now:
      When using filters, you must provide **the exact regular expression pattern** as
      defined in the policies you want to match. For example, if a policy is defined
      with pattern `^secrets/database/production$`, you must use exactly that pattern
      to find it---no partial matches or simpler patterns will work.

      ^ it would be nice for the list command to have substring matches.

      ^ we can then use the same substring match logic to list secrets
      (i.e. list secrets that have db/creds anywhere in their path.
    </issue>
    <issue>
      consider adding an "s3" backing store.

      this will be different from the Lite option. It will support policies, and other utilities,
      it will essentially act as a file-based databased stored in an object store.

      policy management will be done via standard SPIKE policies.

      will be able to connect s3 and anything s3-compatible.

      s3 connection can be established by SPIFFE OIDC, or some other way.
    </issue>
    <issue>
      nexus should have a status endpoint and pilot should warn the user if
      nexus is not ready

      Bootstrap completed successfully!
      âžœ spike git:(feature/no-cache) âœ— spike secret list
      Error listing secret keys: post: Problem connecting to peer
      Post "https://localhost:8553/v1/store/secrets?action=list": dial tcp 127.0.0.1:8553: connect: connection refused
      âžœ spike git:(feature/no-cache) âœ— spike secret list

    </issue>
    <issue>
      fix all `t.Skip()` skipped tests.
    </issue>
    <issue>
      add unit tests.
      we are adding more and more features, and we don't have sufficient
      coverage.
    </issue>
    <issue>
      Test SPIKE Lite setup.
      And maybe create a demo video.
    </issue>
    <issue>
      create a /status endpoint for SPIKE Nexus and use that for the
      Bootstrap job instead of querying the jobs k8s api.
      that will also mean, the Bootstrap job will be more secure since
      its ServiceAccount will not need kube api access.
    </issue>
    <issue>
      maybe for vsecm: plugin-based dynamic secrets to access third-party
      services:
      https://developer.hashicorp.com/vault/tutorials/get-started/understand-static-dynamic-secrets
    </issue>
    <issue>
      if spike nexus had a prometheus endpoint, what kinds of metrics
      would it expose?
    </issue>
    <issue>
      vsecm:
      Integration with Key Vault providers
      - e.g. AWS Secrets Manager, CyberArk
    </issue>
    <issue>
      SPIKE OPA Integration
      ---
      webhooks for external policies, and maybe ValidatingAdmissionPolicies (CEL based)
    </issue>
    <issue>
      Kubernetes secrets as a backing store for SPIKE.
      The secrets are stored in k8s secrets in encrypted form.
      the workloads have to decrypt the secrets using the lite API.
    </issue>
    <issue>
      Audit Logging: SOC 2 Type II compliance
      Records of creation, modification, and deletion, including the user who performed the action and a timestamp.
    </issue>
    <issue>
      add `make audit` to the CI pipeline
      ci does its own verifications, so does `make audit`
      these should be merged.
    </issue>
    <issue>
      make sure we check Spike.LiteWorkload spiffe id in policies.
      also make sure the encryption as a service works.
    </issue>
    <issue>
      A `--dry run` feature for vsecm commands:
      it will not create policies, secrets, etc, but just pass validations
      and return success/failure responses instead.
      useful for integration tests.
    </issue>
    <issue>
      ability for nexus to return encrypted secrets.
      vsecm wants that.
    </issue>
    <issue>
      goes to spike sdk go
      func Id() string {
      id, err := crypto.RandomString(8)
      if err != nil {
      id = fmt.Sprintf("CRYPTO-ERR: %s", err.Error())
      }
      return id
      }
    </issue>
    <issue>
      VSecM should use spiffe.source too from SDK.
      Also spiffe.source should have a timeout and err out if it cannot
      acquire the source in a timely manner. The timeout should be
      configurable from the environment options.
    </issue>
    <issue>
      verify lite mode.
    </issue>
    <issue>
      verify kind scripts.
    </issue>
    <issue>
      vsecm: cleanup experimental parts;
      switch to github registry; switch to %100 helm;
      directly consume spire charts from upstreams
    </issue>
    <issue>
      create a federated spike doc; and also a video.
    </issue>
  </immediate-backlog>
  <runner-up>
    <issue>
      build-local.sh and build-push-sign.sh have a lot of commonalities;
      maybe refactor/merge them?
    </issue>
    <issue>
      VSecM ADR:
      SPIKE Integration Plans
      Create an ADR about the near/mid future plans wrt VSecM and SPIKE.
    </issue>
    <issue>
      SPIKE Documentation
      Generating Protocol Buffers should be before the "build the project" section
      also, needed to do "go mod vendor"
      https://vsecm.com/documentation/development/use-the-source/
    </issue>
    <issue>
      Make vSecM use helm only (you can override images if needed,
      to test with local images. -- that will be better than
      generating local manifests and maintaining them separately)
    </issue>
    <issue kind="v1.0-requirement">
      - Postgres support as a backing store.
      - Postgres should be a plugin (similar to SQLite)
    </issue>
    <issue kind="v1.0-requirement">
      - Ability to channel audit logs to a log aggregator.
      - NOTE: This feature can be delegated to VSecM instead.
    </issue>
    <issue kind="v1.0-requirement">
      - OIDC integration: Ability to connect to an identity provider.
      - NOTE: This feature can be delegated to VSecM instead.
      VSecM can be an identity broker; and SPIKE can be a client.
    </issue>
    <issue kind="v1.0-requirement">
      - ESO (External Secrets Operator) integration
      - NOTE: This feature can be delegated to VSecM to.
    </issue>
    <issue kind="v1.0-requirement">
      - An ADMIN UI (linked to OIDC probably)
      - NOTE: This feature can be delegated to VSecM instead.
      VSecM can provide a UI for SPIKE Nexus.
    </issue>
    <issue kind="v1.0-requirement">
      - Ability to use the RESTful API without needing an SDK.
      That could be hard though since we rely on SPIFFE authentication and
      SPIFFE workload API to gather certs: We can use a tool to automate
      that
      part. But it's not that hard either if I know where my certs are:
      `curl --cert /path/to/svid_cert.pem --key /path/to/svid_key.pem
      https://mtls.example.com/resource`
    </issue>
    <issue kind="v1.0-requirement">
      > 80% unit test coverage
    </issue>
    <issue kind="v1.0-requirement">
      Fuzzing for the user-facing API
    </issue>
    <issue kind="v1.0-requirement">
      100% Integration test (all features will have automated integration
      tests
      in all possible environments)
    </issue>
    <issue kind="user-request,v1.0-requirement">
      Ability to add custom metadata to secrets.
    </issue>
  </runner-up>
  <later>
    <issue>
      thing like
      systemctl start spike-nexus
      systemctl start spike-keeper
      systemctl status spike-nexus
      systemctl status spike-keeper
      --
      also `spike status` from SPIKE Pilot should give a brief status of the
      system... how many keepers, what are the health of the keepers,
      health of nexus, how many secrets, resource usage, etc.
    </issue>
    <issue>
      # to be added to docs.
      # how to expose things to other clusters:
      # kubectl port-forward --address 0.0.0.0 svc/nginx-lb 8080:80
      # no need for ingress
      # no need for `kubectl port-forward`
      # great for demo/development setups.
      # Also forward registry to docker to work
      # kubectl port-forward -n kube-system svc/registry 5000:80
      # You don't need to `eval $(minikube -p minikube docker-env)`
      # Again, this is simpler.
    </issue>
    <issue>
      SPIKE (and also VSecM)
      create github workflow to run tests and coverage report and publish it on
      /public at every merge.
    </issue>
    <issue>
      VSecM: Use GCR.
    </issue>
    <issue>
      VSecM fix:
      func log(message string) {
      conn, err := grpc.Dial(
      SentinelLoggerUrl(),
      grpc.WithTransportCredentials(insecure.NewCredentials()),
      grpc.WithBlock(),
      )
      Dial is deprecated
      WithBlock is deprecated

      // Create a gRPC client
      conn, err := grpc.Dial(lis.Addr().String(), grpc.WithInsecure(), grpc.WithBlock())
      if err != nil {
      t.Fatalf("failed to dial server: %v", err)
      }
      WithInsecure/WithBlock are deprecated at rpc_test.go
    </issue>
    <issue>
      VSecM fix:
      http_test
      nopcloser is deprecated
      func TestReadBody_Success(t *testing.T) {
      // Prepare the test data
      cid := "test-cid"
      expectedBody := []byte("test body content")
      r := &amp;http.Request{
      Body: ioutil.NopCloser(bytes.NewBuffer(expectedBody)),
      }
    </issue>
    <issue>
      VSecM Fix:
      secret-server/main.go
      ReadAll is deprected
    </issue>
    <issue>
      VSecM Fix
      // Read the request body
      body, err := ioutil.ReadAll(r.Body)
      if err != nil {
      http.Error(w, "Cannot read body", http.StatusBadRequest)
      return
      }
      defer r.Body.Close()
      and there are unhandled errors in r.Body.Close() s.
    </issue>
    <issue>
      VSecM Fix

      postgres.go +> remove postgres support; it's better to add it to SPIKE instead.
    </issue>
    <issue>
      VSecM Fix:
      move backoff/retry code to the go sdk.
    </issue>
    <issue>
      VSecM Fix:
      stream := cipher.NewCFBDecrypter(block, iv)
      NewCFBDecrypter is deprecated.
      decrypt.go

      same:
      stream := cipher.NewCFBEncrypter(block, iv)
      stream.XORKeyStream(ciphertext[aes.BlockSize:], []byte(data))
      NewCFBDecrypter is deprecated.
    </issue>
    <issue>
      VSecM Fix:
      remove relay client and relay server-related code.
    </issue>
    <issue>
      VSecM:
      move docs to public and update CloudFlare worker to automatically
      consume it.
    </issue>
    <issue>
      VSecM: need to automate documentation
      right now, we create manual deployments on CloudFlare and that does
      not scale.
      - create a public folder in the repo
      - let cloudflare update the website from the "public" folder.
      - "older versions" are still broken; but since documentation is markdown
      we can always point the tagged version of it as former docs we
      don't need an entire browsable website for it. At least, that's how we
      do it with SPIKE and it saves effort.
    </issue>
    <issue>
      Integration test: Ensure SPIKE Nexus caches the root key in memory.
    </issue>
    <issue>
      Integration test: Ensure SPIKE Nexus recovers root key from keepers.
    </issue>
    <issue>
      Integration test: Ensure SPIKE Nexus does not (inadvertently) initialize twice.
      Once it's initialized successfully it should not recompute root key
      material without manual `spike operator` intervention (because rotating
      the root key without re-encrypting secrets will turn the backing store
      unreadable)
      when the key is lost, it should wait it to be re-seeded by keepers, or
      manually recovered via `spike operator`.
    </issue>
    <issue>
      Integration test: Ensure SPIKE Pilot denies any operation when SPIKE
      Nexus is not initialized.
    </issue>
    <issue>
      Integration test: Ensure SPIKE Pilot warns the user if SPIKE Nexus is
      unreachable.
    </issue>
    <issue>
      Integration tests: Ensure we can create and read secrets.
    </issue>
    <issue>
      Integration test: Ensure we can create and read policies.
    </issue>
    <issue>
      There is an ongoing work on HTML...ization of the Turtle Book.
      We also need to start a work on updating spiffe.io for the
      new book.
    </issue>
    <issue>
      in development mode, nexus shall act as a single binary:
      - you can create secrets and policies via `nexus create policy` etc

      that can be done by sharing
      "github.com/spiffe/spike/app/spike/internal/cmd"
      between nexus and pilot

      this can even be an optional flag on nexus
      (i.e. SPIKE_NEXUS_ENABLE_PILOT_CLI)
      running ./nexus will start a server
      but run
      ning nexus with args will register secrets and policies.
    </issue>
    <issue>
      Consider using OSS Security Scorecard:
      https://github.com/vmware-tanzu/secrets-manager/security/code-scanning/tools/Scorecard/status
    </issue>
    <issue type="security">
      SPIKE automatic rotation of encryption key.
      the shards will create a root key and the root key will encrypt the
      encryption key.
      so SPIKE can rotate the encryption key in the background and encrypt
      it with the new root key.
      this way, we won't have to rotate the shards to rotate the
      encryption key.
    </issue>
    <issue type="enhancement">
      SPIKE CSI Driver

      the CSI Secrets Store driver enables users to create
      `SecretProviderClass` objects. These objects define which secret
      provider
      to use and what secrets to retrieve. When pods requesting CSI
      volumes are
      made, the CSI Secrets Store driver sends the request to the OpenBao
      CSI
      provider if the provider is `vault`. The CSI provider then uses the
      specified `SecretProviderClass` and the podâ€™s service account to
      retrieve
      the secrets from OpenBao and mount them into the podâ€™s CSI volume.
      Note
      that the secret is retrieved from SPIKE Nexus and populated to the
      CSI
      secrets store volume during the `ContainerCreation` phase.
      Therefore, pods
      are blocked from starting until the secrets are read from SPIKE and
      written to the volume.
    </issue>
    <issue type="evaluate">
      shall we implement rate limiting; or should that be out of scope
      (i.e. to be implemented by the user.
    </issue>
    <issue type="enhancement">
      more fine grained policy management

      1. an explicit deny will override allows
      2. have allowed/disallowed/required parameters
      3. etc.

      # This section grants all access on "secret/*". further restrictions
      can be
      # applied to this broad policy, as shown below.
      path "secret/*" {
      capabilities = ["create", "read", "update", "patch", "delete",
      "list", "scan"]
      }

      # Even though we allowed secret/*, this line explicitly denies
      # secret/super-secret. this takes precedence.
      path "secret/super-secret" {
      capabilities = ["deny"]
      }

      # Policies can also specify allowed, disallowed, and required
      parameters. here
      # the key "secret/restricted" can only contain "foo" (any value) and
      "bar" (one
      # of "zip" or "zap").
      path "secret/restricted" {
      capabilities = ["create"]
      allowed_parameters = {
      "foo" = []
      "bar" = ["zip", "zap"]
      }

      but also, instead of going deep down into the policy rabbit hole,
      maybe
      it's better to rely on well-established policy engines like OPA.

      A rego-based evaluation will give allow/deny decisions, which SPIKE
      Nexus
      can then honor.

      Think about pros/cons of each approach. -- SPIKE can have a
      good-enough
      default policy engine, and for more sophisticated functionality we
      can
      leverage OPA.
    </issue>
    <issue type="security">
      key rotation

      NIST rotation guidance

      Periodic rotation of the encryption keys is recommended, even in the
      absence of compromise. Due to the nature of the AES-256-GCM
      encryption
      used, keys should be rotated before approximately 232
      encryptions have been performed, following the guidelines of NIST
      publication 800-38D.

      SPIKE will automatically rotate the backend encryption key prior to
      reaching
      232 encryption operations by default.

      also support manual key rotation
    </issue>
    <issue>
      Do an internal security analysis / threat model for spike.
    </issue>
    <issue>
      TODO in-memory "dev mode" for SPIKE #spike (i.e. in memory mode will
      not be default)
      nexus --dev or something similar (maybe an env var)
    </issue>
    <issue>
      dynamic secrets
    </issue>
    <issue>
      use case:
      one time access to an extremely limited subset of secrets
      (maybe using a one time, or time-bound token)
      but also consider if SPIKE needs tokens at all; I think we can
      piggyback
      most of the authentication to SPIFFE and/or JWT -- having to convert
      various kinds of tokens into internal secrets store tokens is not
      that much needed.
    </issue>
    <issue>
      - TDO Telemetry
      - core system metrics
      - audit log metrics
      - authentication metrics
      - database metrics
      - policy metrics
      - secrets metrics
    </issue>
    <issue>
      "token" secret type
      - will be secure random
      - will have expiration
    </issue>
    <issue>
      double-encryption of nexus-keeper comms (in case mTLS gets
      compromised, or
      SPIRE is configured to use an upstream authority that is
      compromised, this
      will provide end-to-end encryption and an additional layer of
      security
      over
      the existing PKI)
    </issue>
    <issue>
      * Implement strict API access controls:
      * Use mTLS for all API connections
      * Enforce SPIFFE-based authentication
      * Implement rate limiting to prevent brute force attacks
      * Configure request validation:
      * Validate all input parameters
      * Implement request size limits
      * Set appropriate timeout values
      * Audit API usage:
      * Log all API requests
      * Monitor for suspicious patterns
      * Regular review of API access logs
      ----
      * Enable comprehensive auditing:
      * Log all secret access attempts
      * Track configuration changes
      * Monitor system events
      * Implement compliance controls:
      * Regular compliance checks
      * Documentation of security controls
      * Periodic security assessments
      ---
      * Tune for security and performance:
      * Optimize TLS session handling
      * Configure appropriate connection pools
      * Set proper cache sizes
      * Monitor performance metrics:
      * Track response times
      * Monitor error rates
      * Alert on performance degradation
    </issue>
    <issue>
      ability to clone a keeper cluster to another standby keeper cluster
      (for redundancy).
      this way, if the set of keepers become not operational, we can
      hot-switch to the other keeper cluster.
      the assumption here is the redundant keeper cluster either remains
      healthy, or is somehow snapshotted -- since the shards are in
      memory, snapshotting will be hard. -- but stil it's worth thinking
      about.
      an alternative option would be to simplyh increase the number of
      keepers.
    </issue>
    <issue>
      work on the "named admin" feature (using Keycloak as an OIDC
      provider)
      This is required for "named admin" feature.
    </issue>
    <issue>
      Consider using google kms, azure keyvault, and other providers
      (including an external SPIKE deployment) for root key recovery.
      question to consider is whether it's really needed
      second question to consider is what to link kms to (keepers or
      nexus?)
      keepers would be better because we'll back up the shards only then.
      or google kms can be used as an alternative to keepers
      (i.e., store encrypted dek, with the encrypted root key on nexus;
      only kms can decrypt it -- but, to me, it does not provide any
      additional advantage since if you are on the machine, you can talk
      to
      google kms anyway)
    </issue>
    <issue>
      dev mode with "zero" keepers.
    </issue>
    <issue>
      etcd-like watch feature for SPIKE

      one functionality that would be really cool and kind of a game changer I
      think, but would be hard to do with sql, would be something similar to
      etcd/k8s's watches.

      spike watch /foo keyhere -o /tmp/somefile

      would update /tmp/somefile whenever it changes on the server
    </issue>
    <issue>
      idea: custom resources for policies and secrets.
    </issue>
    <issue>
      adding a timeout or circuit breaker to the infinite loop in
      BootstrapBackingStoreWithNewRootKey, I was suggesting a way to prevent
      the function from running indefinitely if something goes wrong with keeper
      initialization.
      The current implementation uses:
      for {
      // Ensure to get a success response from ALL keepers eventually.
      exit := iterateKeepersToBootstrap(
      keepers, rootShares, successfulKeepers, source,
      )
      if exit {
      return
      }

      log.Log().Info(fName, "message", "Waiting for keepers to initialize")
      // TODO: make the time configurable.
      time.Sleep(5 * time.Second)
      }
      This loop will continue forever until all keepers are successfully
      initialized. While this makes sense for normal operation, there are
      scenarios where this could become problematic:

      If one or more keepers are permanently unavailable or unreachable
      If there's a persistent network issue preventing communication
      If there's a configuration issue that makes successful initialization
      impossible

      A potential improvement would be to add:
      maxAttempts := env.GetMaxBootstrapAttempts() // Could be configurable
      attempts := 0

      ctx, cancel := context.WithTimeout(context.Background(),
      env.GetBootstrapTimeout())
      defer cancel()

      for {
      select {
      case lt-ctx.Done():
      log.Log().Warn(fName, "message", "Bootstrap timed out after waiting threshold")
      // Implement fallback strategy or escalate the issue
      return
      default:
      attempts++
      if maxAttempts > 0 ++ attempts > maxAttempts {
      log.Log().Warn(fName, "message", "Exceeded maximum bootstrap attempts")
      // Implement fallback strategy or escalate the issue
      return
      }

      exit := iterateKeepersToBootstrap(
      keepers, rootShares, successfulKeepers, source,
      )
      if exit {
      return
      }

      log.Log().Info(fName, "message", "Waiting for keepers to initialize",
      "attempt", attempts, "maxAttempts", maxAttempts)
      time.Sleep(5 * time.Second)
      }
      }
      This approach provides:

      A maximum number of attempts (configurable)
      A total timeout for the entire operation (configurable)
      Better observability of progress through attempt counting

      In highly reliable systems, you might want the bootstrap to keep trying
      forever, but even then, it's valuable to have observability into how
      long it's been trying and an option to break the loop if needed.
    </issue>
    <issue>
      app/nexus/internal/state/backend/sqlite/persist/crypto.go
      these are generic enough to move to the SDK.
    </issue>
    <issue>
      validateContext(ctx, fName)
      this can be an SDK utility function and can be used across the codebase instead of just this package.

    </issue>
    <issue>
      // maybe add reference to mirrored go docs in the public website too,
      // since the docs are very comprehensive and accurate now.
    </issue>
    <issue>
      pattern-based random secret generation
      VSecM already does it; leverage it from it.
      Or, alternatively, move the code to SPIKE SDK and let VSecM use it
      from SPIKE.
    </issue>
    <issue>
      - TODO optimize sqlite default params and also make sure we retry
      database operations -- at least check that we have sane defaults.
      - ref: https://tenthousandmeters.com/blog/sqlite-concurrent-writes-and-database-is-locked-errors/
    </issue>
    <issue kind="qa">
      test that the timeout results in an error.

      ctx, cancel := context.WithTimeout(
      context.Background(), env.DatabaseOperationTimeout(),
      )
      defer cancel()

      cachedPolicy, err := retry.Do(ctx, func() (*data.Policy, error) {
      return be.LoadPolicy(ctx, id)
      })

    </issue>
    <issue>
      ability to lock nexus programmatically.
      `spike operator lock/unlock` => will need the right clusterspiffeid for
      the command to work.

      ^ instead of that, you can run a script that removes all SVID
      registrations. That will effectively result in the same thing.
    </issue>
    <issue>
      increase unit test coverage.
    </issue>
    <issue priority="medium" severity="medium">
      a way to factory-reset SPIKE: reset db; recreate rootkey; delete
       etc.

      spike operator reset:
      deletes and recreates the ~/.spike folder
      restarts the initialization flow to rekey keepers.

      volkan@spike:~/Desktop/WORKSPACE/spike$ spike secret get /db
      Error reading secret: post: Problem connecting to peer

      ^ I get an error instead of a "secret not found" message.
    </issue>
    <issue priority="medium" severity="medium">
      2025/04/25 13:12:19 Aborting.
      spike (feature/faster-recovery)$ ./hack/bare-metal/entry/spire-server-entry-recover-register.sh

      Also this should be part of operator

      spire-server entry update \
      -entryID "$ENTRY_ID" \
      -spiffeID spiffe://spike.ist/spike/pilot/role/recover \
      -parentID "spiffe://spike.ist/spire-agent" \
      -selector unix:uid:"$(id -u)" \
      -selector unix:path:"$PILOT_PATH" \
      -selector unix:sha256:"$PILOT_SHA"

      as in run the above command where PILOT_PATH is â€¦ â€¦ etc etc.

      should be part of the binary.
      or maybe a separate binary should execute those in an interactive manner.

      create make targets for these.
    </issue>
    <issue priority="high" severity="medium">
      Test with different shamir ratios

      * 5/3 -- 5 keepers, out of 3 should be alive.
      * 1/1 -- A single keeper
      * 0/0 -- edge case; not sure how it should behave.
      * in-memory -- in-memory mode should disregard any keeper config.
    </issue>
    <issue kind="performance,research" severity="low" priority="low" fun="high">
      {"time":"2025-04-25T13:24:52.652299515-07:00","level":"INFO","m":"HydrateMemoryFromBackingStore","m":"HydrateMemoryFromBackingStore:
      secrets loaded"}
      {"time":"2025-04-25T13:24:52.652368182-07:00","level":"INFO","m":"HydrateMemoryFromBackingStore","m":"HydrateMemoryFromBackingStore:
      policies loaded"}
      ^
      how can we know that this data has already been pushed.
      a brute force way is to hash the last payloads and compare with the hashes of the current payloads.
      if hydrated, no need to re-hydrate then.
      but that requires two full table scans, json serialization, and hashing.
      could there be a better way?
    </issue>
  </later>
  <future>
    <issue>
      UpsertPolicy calls LoadAllPolicies to find a policy by name, which is
      O(n) and inefficient as the number of policies grows. Add a
      LoadPolicyByName method to the backend interface that performs an
      indexed lookup directly (SQLite can use an index on the name column).
      This would make the lookup O(1) instead of O(n).

      Location: app/nexus/internal/state/base/policy.go (UpsertPolicy function)
      Backend interface: app/nexus/internal/state/backend/interface.go
    </issue>
    <issue>
      Make vSecM uses helm only (you can override images if needed,
      to test with local images. -- that will be better than
      generating local manifests and maintaining them separately)
    </issue>
    <issue>
      Create VSecM ADR:

      VSecM-SPIKE Integration Strategy

      SPIKE: SPIFFE-native secrets manager
      VSecM: Secrets Manager and Orchestrator

      SPIKE: mandatory SPIFFE use
      VSecM: SPIFFE is a core feature but can work/integrate with others

      SPIKE: minimal CLI as UX structure (and API)
      VSecM: policy engine, audits, secret LCM, automation, UI-ready.

      SPIKE: embedded, edge, stateless
      VSecM: stateful, policy-aware, multi-tenant

      Helm:
      SPIKE: subchart under SPIFFE
      VSecM: consumes SPIRE helm chart; can enable SPIKE
      * SPIKE is set up as the "default" secrets manager and first-class
      integration; but it can integrate with other secrets stores too.
    </issue>
    <issue>
      GitHub actions is creating pipeline errors:
      https://github.com/vmware/secrets-manager/settings/code-scanning/default-setup
    </issue>
    <issue>
      VSecM:
      For Ubuntu users; do not use snap to install docker as it can create permission issues when working with minikube.
      */
      ^
      rendering on the page has error.
    </issue>
    <issue>
      VSecM:
      consume SPIRE from upstream helm instead of our custom fork.
    </issue>
    <issue>
      maybe a default auditor SPIFFEID that can only read stuff (for
      Pilot;
      not for named admins; named admins will use the policy system
      instead)
    </issue>
    <issue>
      document limits and maximums of SPIKE (such as key length, path
      length, policy size etc)

      also ensure that in the source code.
    </issue>
    <issue>
      We need use cases in the website
      - Policy-based access control for workloads
      - Secret CRUD operations
      - etc
    </issue>
    <issue when="future" reason="no immediate product value">
      get an OpenSSF badge sometime.
    </issue>
    <issue>
      OIDC authentication for named admins.
    </issue>
    <issue>
      SPIKE Dynamic secret sidecar injector
    </issue>
    <issue>
      maybe ha mode

      HA Mode in OpenBao: In HA mode, OpenBao operates with one active server
      and multiple standby servers. The active server processes all requests,
      while standby servers redirect requests to the active instance. If the
      active server fails, one of the standby servers takes over as the new
      active instance. This mechanism relies on PostgreSQL's ability to manage
      locks and ensure consistency across nodes35.
      Limitations:
      The PostgreSQL backend for OpenBao is community-supported and considered
      in an early preview stage, meaning it may have breaking changes or limited
      testing in production environments2.
      While PostgreSQL supports replication and failover mechanisms for its own
      HA, these features operate independently of OpenBao's HA mode. Proper
      configuration and monitoring of the PostgreSQL cluster are essential to
      ensure database-level resilience
    </issue>
    <issue>
      v.1.0.0 Requirements:
      - Having S3 as a backing store
      - This is different than SPIKE Lite
    </issue>
    <issue>
      Consider a health check / heartbeat between Nexus and Keeper.
      This can be more frequent than the root key sync interval.
    </issue>
    <issue>
      Unit tests and coverage reports.
      Create a solid integration test before.
    </issue>
    <issue>
      Test automation.
    </issue>
    <issue>
      double encryption when passing secrets around
      (can be optional for client-nexus interaction; and can be mandatory
      for
      tools that transcend trust boundaries (as in a relay / message queue
      that
      may be used for secrets federation)
    </issue>
    <issue>
      active/standby HA mode
    </issue>
    <issue>
      audit targets:
      - file
      - syslog
      - socket
      (if audit targets are enabled then command will not execute unless
      an
      audit trail is started)
    </issue>
    <issue>
      admin ui
    </issue>
    <issue>
      - AWS KMS support for keepers
      - Azure keyvault support for keepers
      - GCP kms support for keepers
      - HSM support for keepers
      - OCI kms support for keepers
      - keepers storing their shards in a separate SPIKE deployment
      (i.e. SPIKE using another SPIKE to restore root token)
    </issue>
    <issue kind="enhancement">
      attribute-based policy control

      path "secret/restricted" {
      capabilities = ["create"]
      allowed_parameters = {
      "foo" = []
      "bar" = ["zip", "zap"]
      }
      }
    </issue>
    <issue>
      spike dev mode:
      - it will not require SPIFFE
      - it will be in memory
      - it will be a single binary
      - it will present a SPIKE Nexus API in that binary.
      - regular unsafe `curl` would work.
      - would be SDK-compatible.

      ^ not sure it's worth the effort, but it will be nice-to-have.
    </issue>
    <issue>
      Note: this is non-trivial, but doable.

      Periodic rotation of the encryption keys is recommended, even in the
      absence of compromise. Due to the nature of the AES-256-GCM
      encryption
      used, keys should be rotated before approximately 232 encryptions
      have
      been performed, following the guidelines of NIST publication
      800-38D.

      This can be achieved by having a separate encryption key protected
      by
      the root key and rotating the encryption key, and maybe maintaining
      a
      keyring. This way, we won't have to rotate shards to rotate the
      encryption
      key and won't need to change the shards -- this will also allow the
      encryption key to be rotated behind-the-scenes automatically as per
      NIST guidance.
    </issue>
    <issue>
      better play with OIDC and keycloak sometime.
    </issue>
    <issue>
      wrt: secure erasing shards and the root key >>
      It would be interesting to try and chat with some of the folks under
      the cncf
      (That's a good idea indeed; I'm noting it down.)
    </issue>
    <issue>
      over the break, I dusted off
      https://github.com/spiffe/helm-charts-hardened/pull/166 and started
      playing with the new k8s built in cel based mutation functionality.
      the k8s cel support is a little rough, but I was able to do a whole
      lot in it, and think I can probably get it to work for everything.
      once 1.33 hits, I think it will be even easier.
      I mention this, as I think spike may want similar functionality?
      csi driver, specify secrets to fetch to volume automatically, keep
      it up to date, and maybe poke the process once refreshed
    </issue>
    <issue>
      consider using NATS for cross trust boundary (or nor) secret
      federation
      note: NATS had licensing issues recently.
    </issue>
    <issue kind="research">
      spike, but as an http proxy?
      like, point http(s)_proxy at this thing, connect to it using your spiffe id, the proxy can use policies and the
      client spiffeid to allow/disallow an http proxy request
      No; but not a bad idea indeed. -- will look around; I'll share if I find something related.
    </issue>
    <issue>
      Check if we need traversal resistant file api needs anywhere.
      SPIKE: traversal-resistant file apis: https://go.dev/blog/osroot
    </issue>
    <issue>
      all components shall have
      liveness and readiness endpoints
      (or maybe we can design it once we k8s...ify things.
    </issue>
    <issue priority="important" severity="medium">
      if a keeper crashes it has to wait for the next nexus cycle which is
      suboptimal. Instead, nexus can send a ping that returns an overall
      status
      of keeper (i.e. if it's populated or not)
      this can be more frequent than hydration; and once nexus realizes
      keeper
      is down, it can rehydrate it.

      in addition; nexus can first check the sha hash of the keeper's shard.
      before resending; if the hashes match, it won't restransmit the shard.
    </issue>
    <issue>
      A /stats endpoint.

      A dedicated /stats endpoint will be implemented to provide real-time
      metrics about:
      Total number of secrets managed.
      Status of the key-value store.
      Resource utilization metrics (e.g., CPU, memory).
      This endpoint will support integration with monitoring tools for enhanced
      observability.
      These measures will ensure comprehensive monitoring and troubleshooting.
    </issue>
    <issue>
      configure SPIKE to rekey itself as per NIST guidelines.
      Also maybe `spike operator rekey` to manually initiate that.
      `spike operator rekey` will also change the shamir shares, wheras the
      internal rekey will just change the encryption key, leaving the shamir
      shares intact.
    </issue>
    <issue>
      verify if the keeper has shard before resending it:
      send hash of the shard first
      if keeper says â€œI have itâ€, donâ€™t send the actual shard.
      this will make things extra secure.
    </issue>
    <issue>
      Fleet management:
      - There is a management plane cluster
      - There is a control plane cluster
      - There are workload clusters connected to the control plane
      - All of those are their own trust domains.
      - There is MP-CP connectivity
      - There is CP-WL connectivity
      - MP has a central secrets store
      - WL and CP need secrets
      - Securely dispatch them without "ever" using Kubernetes secrets.
      - Have an alternative that uses ESO and a restricted secrets
      namespace
      that no one other than SPIKE components can see into.
    </issue>
    <issue>
      aes256-cbc
      jay:U2FsdGVkX1+VhdGia1yk+JAUSraXj60ZA2ydT9TuHmQBE1sWLcMLb5z0B76sCqpJVGi1GQCl8BnnoV5kznYneQ==
    </issue>
    <issue>
      TDO it's early but have a deprecation policy for SPIKE
      https://external-secrets.io/latest/introduction/deprecation-policy/
    </issue>
    <issue>
      TOO update vsecm documentation about cloudflare changes 1. manually upload 2. point to github for older versions.
    </issue>
    <issue>
      Consider metrics collector integration.

      This is from SPIRE:
      The metrics collectors that are currently supported are Prometheus, Statsd,
      DogStatsd, and M3. Multiple collectors can be conï¬gured simultaneously, both in
      the servers and the agents.

      Think about what telemetry SPIKE can create.

      create telemetry data and support certain metrics collectors.

      create a monitoring epic
    </issue>
    <issue priority="high" severity="low">
      [vmware-tanzu/secrets-manager] Scorecard supply-chain security workflow run
      ^
      this is constantly failing.
      maybe disable it.

      also it's about time to cut a VSecM release since we had quite a few
      security patches in already.
    </issue>
    <issue for="vsecm">
      VSecM: Maybe use SPIFFE Helper instead of VSecM sidecar since it
      essentially does the same thing. Or maybe have an alternative
      implementation that uses spiffe helper instead of vsecm sidecar.
    </issue>
    <issue>
      Use Case: Each SPIKE Keeper in its own trust domain.
      (we can also create a demo once this feature is established)

      Details:

      The current design assumes that all keepers are in the same trust boundary
      (which defaults to spike.ist)
      that can make sense for a single-cluster deployment
      (each keeper can be tainted to deploy itself on a distinct node for
      redundancy)
      however; a keeper's SPIFFE ID does not have to have a single trust root.
      each SPIKE keeper can (in theory) live in a separate cluster, in its own
      trust boundary, depicted by a different trust root.

      If that's the case the below function will not be valid

      func IsKeeper(id string) bool {
      return id == spiffeid.SpikeKeeper()
      }

      Instead of this the validation should be done against SPIKE_NEXUS_KEEPER_PEERS
      and the env var should also contain the trust root of each keeper.

      Similarly, the function cannot check the trust root.
      it may however verify the part of spiffeID "after" the trust root.

      // I should be a SPIKE Keeper.
      if !cfg.IsKeeper(selfSpiffeid) {
      log.FatalF("Authenticate: SPIFFE ID %s is not valid.\n", selfSpiffeid)
      }

      So for the `main` function of SPIKE Keeper we'll need a more relaxed
      version of IsKeeper.
      and the IsKeeper in SPIKE Nexus will validate looking at the env
      config.

      which also means, SPIKE Keeper can require SPIKE_TRUST_ROOT along
      with SPIKE_KEEPER_TLS_PORT to start. at start-keeper-1.sh
    </issue>
    <issue kind="idea">
      ideation:

      state is expensive to maintain.
      thats one of the reasons cloud services try and decouple/minimize
      state
      as such, the fewest number of state stores I can get away with
      reasonably the better
      and the state stores that are light weight are much better then the
      state stores that are heavy weight.
      there is no more heavyyweight state store than a network attached
      sql sever.


      It makes sense to argue that "we already have paid the expensive
      cost of a postgresql,
      so we just want to use that rather then add anything else". That,
      can make sense.

      but for those of us not carrying a postgresql already, its better
      not to have to have one added.

      so... it makes sense to make the backing store "plugin-based"

      3 backends that people might want for different reasons:
      * s3 - be stateless for your own admin needs, state managed by
      someone else
      * k8s crds - you are already maintaining an etcd cluster. Might as
      well reuse it
      * postgresql - you maintain a postgresql and want to reuse that

      the first two initially feel different then postgresql code wise...
      they are document stores.
      But posgres is pretty much json-compatible; besides SPIKE does not
      have a complicated ata model.
      So, we can find a common ground and treat all databases that are
      plugged-in as forms of document stores.

      It could keep the code to talk to the db to a real minimum.

      The files should be encrypted by the spike key, so should be fine
      just putting in a k8s crd or something without a second layer of
      encryption
      that can be a big selling point. already have a small k8s cluster?
      just add this component and now you have a secret store too. no
      hassle.
    </issue>
    <issue kind="idea">
      use custom resources as backing store;
      since everything is encrypted and not many people want a fast
      secrets creation throughtput it woudl be useful.
      because then you can do `helm install spiffe/spire` and use it
      without any state tracking.
    </issue>
    <issue kind="idea">
      for k8s instructions (docs)
      Might recommend deploying with the chart rather then from scratch,
      which has a lot of those settings. then we can call out the settings
      in the chart on how to do it
    </issue>
    <issue>
      An external secrets store (such as Hashi Vault) can use SPIKE Nexus
      to
      auto-unseal itself.
    </issue>
    <issue>
      multiple keeper clusters:

      keepers:
      - nodes: [n1, n2, n3, n4, n5]
      - nodes: [dr1, dr2]

      if it cant assemble back from the first pool, it could try the next
      pool, which could be stood up only during disaster recovery.
    </issue>
    <issue>
      a tool to read from one cluster of keepers to hydrate a different
      cluster of keepers.
    </issue>
    <issue>
      since OPA knows REST, can we expose a policy evaluation endpoint to
      help OPA augment/extend SPIKE policy decisions?
    </issue>
    <issue>
      maybe create an interface for kv, so we can have thread-safe
      variants too.
    </issue>
    <issue>
      maybe create a password manager tool as an example use case
    </issue>
    <issue>
      A `stats` endpoint to show the overall
      system utilization
      (how many secrets; how much memory, etc)
    </issue>
    <issue>
      maybe inspire admin UI from keybase
      https://keybase.io/v0lk4n/devices
      for that, we need an admin ui first :)
      for that we need keycloak to experiment with first.
    </issue>
    <issue>
      the current docs are good and all but they are not good for seo; we
      might
      want to convert to something like zola later down the line
    </issue>
    <issues>
      wrt ADR-0014:
      Maybe we should use something S3-compatible as primary storage
      instead of sqlite.
      But that can wait until we implement other features.

      Besides, Postgres support will be something that some of the
      stakeholders
      want to see too.
    </issues>
    <issue>
      SPIKE Dev Mode:

      * Single binary
      * `keeper` functionality runs in memory
      * `nexus` uses an in-memory store, and its functionality is in the
      single
      binary too.
      * only networking is between the binary and SPIRE Agent.
      * For development only.

      The design should be maintainable with code reuse and should not
      turn into
      maintaining two separate projects.
    </issue>
    <issue>
      rate limiting to api endpoints.
    </issue>
    <issue>
      * super admin can create regular admins and other super admins.
      * super admin can assign backup admins.
      (see drafts.txt for more details)
    </issue>
    <issue>
      Each keeper is backed by a TPM.
    </issue>
    <issue>
      Do some static analysis.
    </issue>
    <to-plan>
      <issue>
        S3 (or compatible) backing store
      </issue>
      <issue>
        File-based backing store
      </issue>
      <issue>
        In memory backing store
      </issue>
      <issue>
        Kubernetes Deployment
      </issue>
    </to-plan>
    <issue>
      - Security Measures (SPIKE Nexus)
      - Encrypting the root key with admin password is good
      Consider adding salt to the password encryption
      - Maybe add a key rotation mechanism for the future
    </issue>
    <issue>
      - Error Handling
      - Good use of exponential retries
      - Consider adding specific error types/codes for different failure
      scenarios
      - Might want to add cleanup steps for partial initialization
      failures
    </issue>
    <issue>
      Ability to stream logs and audit trails outside of std out.
    </issue>
    <issue>
      Audit logs should write to a separate location.
    </issue>
    <issue>
      Create a dedicated OIDC resource server (that acts like Pilot but
      exposes
      a
      restful API for things like CI/CD integration.
    </issue>
    <issue>
      HSM integration (i.e. root key is managed/provided by an HSM, and
      the key
      ever leaves the trust boundary of the HSM.
    </issue>
    <issue>
      Ability to rotate the root key (automatic via Nexus).
    </issue>
    <issue>
      Ability to rotate the admin token (manual).
    </issue>
    <issue>
      Encourage to create users instead of relying on the system user.
    </issue>
  </future>
  <performance-optimization>
    <issue priority="high" category="performance">
      Implement in-memory policy caching with cache invalidation.

      PROBLEM:
      Currently, every authorization check (CheckAccess) performs a full
      database query to load ALL policies, decrypts them, and compiles
      regex patterns. This causes significant performance overhead:
      - Database query on every authorization check
      - Decryption of all policies on every check
      - Regex compilation on every check
      - High database load in high-volume systems

      SOLUTION:
      Implement an in-memory policy cache with the following design:

      1. Cache Structure:
         - Concurrent-safe map (sync.RWMutex or sync.Map)
         - Store compiled Policy objects with pre-compiled regex patterns
         - Key: policy ID, Value: *data.Policy (with IDRegex, PathRegex)

      2. Cache Population:
         - Load policies from database at Nexus startup
         - Decrypt and compile regex patterns once during load
         - Store in cache for fast access

      3. Cache Invalidation:
         - Invalidate entire cache on UpsertPolicy
         - Invalidate specific entry on DeletePolicy
         - Reload policies from database after invalidation

      4. Authorization Flow:
         - CheckAccess reads from cache (no database query)
         - Use pre-compiled regex patterns for fast matching
         - No decryption overhead

      EXPECTED PERFORMANCE IMPROVEMENT:
      - Policy checks: From milliseconds to microseconds
      - Eliminates database query per authorization check
      - Eliminates decryption overhead per check
      - Eliminates regex compilation overhead per check

      FILES TO MODIFY:
      - app/nexus/internal/state/base/policy.go: CheckAccess, ListPolicies
      - app/nexus/internal/state/base/global.go: Add policy cache variable
      - app/nexus/internal/state/persist/global.go: Cache management

      RELATED:
      - Documentation incorrectly claimed policy caching existed
      - Fixed in commit: [to be added]
      - Diagrams updated to reflect actual behavior

      TESTING:
      - Unit tests for cache invalidation
      - Concurrent access tests (race detector)
      - Performance benchmarks before/after
    </issue>
  </performance-optimization>
  <security>
    <issue priority="low-medium" category="security-crypto">
      Fix AES-GCM nonce reuse vulnerability in policy encryption.

      VULNERABILITY:
      Policy encryption reuses the same nonce for encrypting three
      different fields (SPIFFE ID pattern, path pattern, permissions)
      within a single policy. This violates the fundamental security
      requirement of AES-GCM: never reuse a (key, nonce) pair for
      different plaintexts.

      LOCATION:
      app/nexus/internal/state/backend/sqlite/persist/policy.go:132-167

      CURRENT CODE:
      ```go
      nonce, nonceErr := generateNonce(s)  // One nonce
      encryptedSpiffeID := encryptWithNonce(s, nonce, spiffeID)
      encryptedPathPattern := encryptWithNonce(s, nonce, pathPattern)
      encryptedPermissions := encryptWithNonce(s, nonce, permissions)
      ```

      SECURITY IMPACT:
      1. Confidentiality: Attacker can XOR ciphertexts to derive
         C1 âŠ• C2 = P1 âŠ• P2, revealing plaintext relationships

      2. Known-plaintext attack: If attacker can guess one field
         (e.g., common SPIFFE ID patterns), they can recover the
         keystream and decrypt other fields

      3. Authentication: GCM authentication guarantees weakened

      4. Pattern analysis: Repeated patterns in policies become
         visible through ciphertext analysis

      SEVERITY: LOW-MEDIUM (Revised from initial CRITICAL assessment)

      REALISTIC THREAT ASSESSMENT:
      - Requires database file access (already inside trust boundary)
      - Limited to 3 fields within a single policy (not cumulative)
      - Policy metadata is structural info, not high-value secrets
      - No cross-policy attack surface (each policy has unique nonce)
      - XOR attack reveals P1âŠ•P2, but with only 3 short fields,
        information leakage is minimal
      - If attacker has DB access, they likely already know policy
        structure from context
      - Defense-in-depth: file permissions, access control, encrypted
        storage

      COMPARISON TO BEST PRACTICES:
      - Violates AES-GCM strict nonce uniqueness requirement
      - Would fail cryptographic audit
      - Should be fixed for compliance and best practices
      - NOT an immediate security emergency

      RECOMMENDATION:
      - Document the issue (done)
      - Fix during next schema migration (not urgent)
      - Acceptable for current deployments given limited impact
      - Include fix in next major version

      SOLUTION:
      Use unique nonce for each field:

      ```go
      nonce1, _ := generateNonce(s)
      encryptedSpiffeID := encryptWithNonce(s, nonce1, spiffeID)

      nonce2, _ := generateNonce(s)
      encryptedPathPattern := encryptWithNonce(s, nonce2, pathPattern)

      nonce3, _ := generateNonce(s)
      encryptedPermissions := encryptWithNonce(s, nonce3, permissions)

      // Store all three nonces
      ```

      DATABASE SCHEMA CHANGES REQUIRED:
      Add two columns to policies table:
      - nonce_path_pattern BLOB
      - nonce_permissions BLOB

      Current schema only has single 'nonce' column.
      Rename to 'nonce_spiffe_id_pattern' for clarity.

      MIGRATION STRATEGY:
      1. Add new columns (nullable initially)
      2. Migration script: Re-encrypt all existing policies with
         three nonces
      3. Make new columns NOT NULL
      4. Update code to use three nonces
      5. Test decryption of old and new policies

      FILES TO MODIFY:
      - app/nexus/internal/state/backend/sqlite/ddl/statements.go
      - app/nexus/internal/state/backend/sqlite/persist/policy.go
      - Migration script for existing policies

      TESTING:
      - Verify each field uses unique nonce
      - Test migration from old schema to new schema
      - Cryptographic tests: verify no nonce reuse
      - Round-trip encryption/decryption tests

      REFERENCES:
      - NIST SP 800-38D (GCM): "The total number of invocations of
        the authenticated encryption function shall not exceed 2^32"
        for a given (key, nonce) pair
      - RFC 5116: "Nonce requirements: The nonce MUST be unique"

      RELATED:
      - Documentation updated to note this security issue
      - See claude_diagrams_07_encryption_flows.md Section 6
    </issue>
  </security>
  <feature-request>
    <issue priority="medium" category="api-feature">
      Add hard delete (destroy) API for secrets, following Vault KV2 model.

      CURRENT STATE:
      SPIKE only supports soft delete for secrets:
      - `spike secret delete path` sets `deleted_time` (soft delete)
      - `spike secret undelete path` clears `deleted_time` (restore)
      - Physical deletion only happens automatically during version pruning
        when MaxVersions is exceeded

      VAULT KV2 COMPARISON:
      HashiCorp Vault KV2 provides two-tier deletion:
      1. `vault kv delete` - Soft delete (marks as deleted, recoverable)
      2. `vault kv destroy` - Hard delete (permanent, irrecoverable)

      This gives operators explicit control over data permanence.

      PROPOSED FEATURE:
      Add `spike secret destroy` command (or `--destroy` flag):

      CLI:
      ```bash
      # Soft delete (current behavior)
      spike secret delete secrets/db/creds

      # Hard delete (new feature)
      spike secret destroy secrets/db/creds --versions 1,2,3

      # Or alternative flag syntax
      spike secret delete secrets/db/creds --destroy --versions 1,2,3
      ```

      API:
      POST /v1/secrets/destroy
      {
        "path": "secrets/db/creds",
        "versions": [1, 2, 3]  // Empty array = destroy all versions
      }

      BEHAVIOR:
      - Physically removes version data from database (DELETE FROM secrets)
      - Updates secret_metadata (adjust oldest_version, current_version)
      - If all versions destroyed, optionally remove metadata row
      - Irreversible - cannot be undone with undelete
      - Requires "delete" or new "destroy" permission in policies

      USE CASES:

      1. **Compliance & Data Governance:**
         - GDPR "right to be forgotten" requirements
         - Data retention policies requiring permanent deletion
         - Regulatory requirements for data sanitization
         - Audit trails showing explicit destruction

      2. **Security Incident Response:**
         - Compromised credentials must be eliminated beyond recovery
         - Leaked secrets need permanent removal
         - Prevent potential forensic recovery of sensitive data

      3. **Storage Management:**
         - Explicitly free up database space
         - Remove secrets that will never be needed again
         - Clean up test/development data permanently

      4. **Operational Clarity:**
         - Clear intent: "never recover this" vs "temporarily hide this"
         - Distinguish between "hide from view" and "erase permanently"
         - Align with industry standard (Vault KV2 behavior)

      DESIGN CONSIDERATIONS:

      1. **Safety Mechanisms:**
         - Require explicit version numbers (no "destroy all" without
           confirmation)
         - Add `--force` flag for non-interactive destruction
         - Log destruction events prominently (audit trail)
         - Consider "cool-down" period or delayed destruction option

      2. **Permission Model:**
         Option A: Reuse "delete" permission (destroy is just stronger
         delete)
         Option B: Add new "destroy" permission (finer-grained control)
         Recommendation: Option B for better separation of concerns

      3. **Metadata Handling:**
         - If all versions destroyed, keep metadata with empty versions
           list?
         - Or completely remove metadata row (path disappears entirely)?
         - Recommendation: Keep metadata skeleton for audit trail

      4. **Interaction with Pruning:**
         - Pruning continues to work automatically
         - Destroyed versions are already gone (no conflict)
         - Update MaxVersions logic to handle empty/sparse version sets

      IMPLEMENTATION PLAN:

      Phase 1: Backend support
      - Add DestroySecret() function in app/nexus/internal/state/base/
      - Implement physical deletion in SQLite backend
      - Add tests for destruction scenarios

      Phase 2: API endpoint
      - Add POST /v1/secrets/destroy route
      - Add policy checks (new "destroy" permission or reuse "delete")
      - Add request validation and error handling

      Phase 3: CLI command
      - Add `spike secret destroy` command
      - Add --versions, --force flags
      - Add confirmation prompts for safety

      Phase 4: Documentation
      - Update API docs
      - Update secret lifecycle diagrams (add destroy state)
      - Add examples and best practices
      - Document permission requirements

      FILES TO MODIFY:
      - app/nexus/internal/state/base/secret.go: Add DestroySecret()
      - app/nexus/internal/state/backend/sqlite/persist/secret.go:
        Physical deletion
      - app/nexus/internal/route/secret/destroy.go: New route handler
      - app/spike/internal/cmd/secret/destroy.go: New CLI command
      - Policy permission model (add "destroy" permission?)

      ALTERNATIVE CONSIDERED:
      Keep current model (soft delete + automatic pruning only)

      Pros:
      - Simpler model, less API surface
      - Prevents accidental permanent data loss
      - Automatic cleanup handles most cases

      Cons:
      - No explicit control for compliance scenarios
      - Diverges from industry standard (Vault KV2)
      - Cannot immediately free up space on demand
      - Harder to prove data was permanently deleted (audit/compliance)

      RECOMMENDATION:
      Implement the feature. Benefits for compliance, security incidents,
      and alignment with Vault KV2 outweigh the added complexity. Add
      safety mechanisms (confirmation, audit logs) to prevent accidents.

      RELATED:
      - Documentation corrected to remove references to non-existent
        `--hard` flag
      - See claude_diagrams_11_secret_lifecycle.md
      - Similar to Vault KV2 destroy command
    </issue>
  </feature-request>
  <documentation>
    <issue priority="low" category="security-clarification">
      Document SPIFFE trust model: Workloads with multiple SVIDs

      CONTEXT:
      A workload can be registered in SPIRE with multiple SPIFFE IDs
      (multiple registration entries matching the same selectors). The
      SPIRE Workload API returns multiple X.509-SVIDs to such workloads,
      and the workload chooses which SVID to present during mTLS connections.

      EXAMPLE:
      Registration entries with same selectors:
      ```bash
      # Entry 1
      spire-server entry create \
        -spiffeID spiffe://example.org/app/lowprivilege \
        -selector unix:uid:1000 \
        -hint internal

      # Entry 2
      spire-server entry create \
        -spiffeID spiffe://example.org/app/admin \
        -selector unix:uid:1000 \
        -hint external
      ```

      Same workload (UID 1000) receives BOTH SVIDs and can choose which
      to present when connecting to SPIKE Nexus.

      SPIKE's BEHAVIOR:
      - SPIKE extracts SPIFFE ID from presented client certificate
        (via spiffe.IDFromRequest -> IDFromCert)
      - SPIKE enforces policies based on that SPIFFE ID
      - SPIKE has NO control over which SVID the client chooses
      - SPIKE trusts that SPIRE only issues SVIDs to authorized workloads

      TRUST MODEL:
      SPIKE's security relies on:
      1. SPIRE correctly attesting workload identity (via selectors)
      2. SPIRE only issuing SVIDs to authorized workloads
      3. Cryptographic validation of presented certificates
      4. Operators configuring SPIRE registration correctly

      SPIKE does NOT:
      - Query SPIRE to verify "should this workload have this SPIFFE ID?"
      - Re-implement workload attestation
      - Restrict which SVID a multi-SVID workload can use

      WHY THIS IS CORRECT:
      - Separation of concerns: Identity issuance (SPIRE) vs authorization
        (SPIKE)
      - If SPIRE issued an SVID, the workload IS authorized for that
        identity
      - SPIFFE trust model: Trust the issuing authority (SPIRE)
      - Industry standard practice (same as Vault, Istio, etc.)

      POTENTIAL CONFUSION:
      Operators might think:
      - "Workload should only use one identity at a time"
      - "SPIKE should prevent workload from using admin SVID"
      - "Multiple SVIDs create privilege escalation risk"

      CLARIFICATION:
      - If a workload has multiple SVIDs, operator INTENDED this
      - Hint field guides workload choice, doesn't restrict it
      - If undesired, fix SPIRE configuration (different selectors)

      OPERATIONAL GUIDANCE:

      Best Practices:
      1. Use different selectors for different privilege levels:
         âœ… Admin workload: kubernetes:sa:admin-service
         âœ… User workload: kubernetes:sa:user-service
         âŒ Both: unix:uid:1000 (too broad)

      2. Avoid multiple high/low privilege SVIDs for same selectors
         unless intentional (e.g., internal/external routing)

      3. Use SPIRE's hint field for intended purpose:
         - Guide workload behavior
         - Document identity purpose
         - NOT for access control (that's SPIKE's job)

      4. Audit SPIRE registration entries:
         - Check for overlapping selectors
         - Verify selector specificity
         - Review multiple entries per workload

      DOCUMENTATION TO ADD:

      Location: docs-src/content/operations/security.md (or similar)

      Section: "SPIFFE Trust Model and Multiple SVIDs"

      Content:
      - Explain SPIKE trusts SPIRE for identity issuance
      - Describe how multiple SVIDs work
      - Clarify SPIKE's role vs SPIRE's role
      - Provide selector best practices
      - Show examples of correct/incorrect configurations
      - Explain hint field purpose

      Location: docs-src/content/architecture/security.md

      Section: "Identity Verification"

      Content:
      - Document that SPIKE extracts SPIFFE ID from client cert
      - Note that workload chooses which SVID to present
      - Explain trust boundary between SPIKE and SPIRE
      - Reference SPIFFE X.509-SVID standard

      RELATED SPIFFE SPEC:

      X.509-SVID Standard:
      - "An X.509 SVID MUST contain exactly one URI SAN"
      - Each SVID = one identity
      - Multiple SVIDs = multiple separate certificates

      Workload API Standard:
      - X509SVIDResponse can contain multiple X509SVID messages
      - Hint field: "operator-specified string to provide guidance"
      - "First SVID in list is default identity"
      - Workload responsible for SVID selection

      NOT A SECURITY BUG:
      This is working as designed per SPIFFE standard. SPIKE correctly
      validates presented certificates and enforces policies. The
      "risk" of multiple SVIDs is an operational configuration concern,
      not a code defect.

      ACTION ITEMS:
      1. Document SPIFFE trust model in security docs
      2. Add section on multiple SVIDs and workload behavior
      3. Provide SPIRE registration best practices
      4. Consider adding debug logging: "Authenticated as SPIFFE ID X"
      5. Maybe add warning if unusual patterns detected (optional)

      OPTIONAL ENHANCEMENT (Future):
      Could add SPIRE Server API integration to cross-check registration
      entries, but this:
      - Adds complexity and coupling
      - Duplicates SPIRE's attestation
      - Goes against SPIFFE separation of concerns
      - Not recommended unless specific compliance requirement

      PRIORITY: Low
      - Not a bug, working as designed
      - Documentation/education issue
      - Helps operators understand trust model
    </issue>
  </documentation>
  <examples>
    <issue priority="medium" category="demo-application">
      Create advanced demo workload showcasing multiple SVIDs with hint-based
      selection.

      PURPOSE:
      Demonstrate SPIFFE's multiple-SVID-per-workload feature and educate
      operators on proper SPIRE configuration, hint field usage, and SPIKE's
      trust model.

      MOTIVATION:
      - Multiple SVIDs per workload is a valid SPIFFE pattern but rarely
        demonstrated
      - Operators may be confused about how SPIKE handles this scenario
      - Good opportunity to show go-spiffe's WithDefaultX509SVIDPicker
      - Illustrates SPIKE's trust boundary and SPIRE's role

      DEMO SCENARIO:
      "Multi-Environment Service Router"

      A single workload that routes requests to internal/external services
      and needs different identities for each:

      Use Cases:
      1. Internal requests â†’ Use "internal" SVID to access internal secrets
      2. External requests â†’ Use "external" SVID to access public secrets
      3. Admin operations â†’ Use "admin" SVID for sensitive operations

      IMPLEMENTATION:

      Demo Application: "spike-demo-multi-identity"

      Location: examples/multi-identity-workload/

      Components:
      1. Go application using go-spiffe
      2. SPIRE registration scripts
      3. SPIKE policy configuration
      4. README with explanation
      5. Docker/Kubernetes deployment manifests

      Code Structure:
      ```
      examples/multi-identity-workload/
      â”œâ”€â”€ README.md                    # Detailed explanation
      â”œâ”€â”€ main.go                      # Demo application
      â”œâ”€â”€ spire-setup.sh               # Register multiple identities
      â”œâ”€â”€ spike-policies.sh            # Create SPIKE policies
      â”œâ”€â”€ docker-compose.yml           # Bare metal demo
      â”œâ”€â”€ k8s/
      â”‚   â”œâ”€â”€ deployment.yaml
      â”‚   â””â”€â”€ spire-entries.yaml
      â””â”€â”€ docs/
          â””â”€â”€ architecture.md          # Trust model explanation
      ```

      APPLICATION BEHAVIOR:

      main.go pseudo-code:
      ```go
      package main

      import (
          "context"
          "github.com/spiffe/go-spiffe/v2/workloadapi"
          "github.com/spiffe/go-spiffe/v2/svid/x509svid"
      )

      func main() {
          ctx := context.Background()

          // Demonstrate fetching multiple SVIDs
          source, _ := workloadapi.NewX509Source(ctx)
          defer source.Close()

          // Get all SVIDs
          x509Ctx, _ := source.GetX509Context()
          fmt.Printf("Received %d SVIDs:\n", len(x509Ctx.SVIDs))
          for _, svid := range x509Ctx.SVIDs {
              fmt.Printf("  - %s (hint: %q)\n", svid.ID, svid.Hint)
          }

          // Route 1: Internal operations
          internalClient := createClientWithHint("internal")
          secret1, _ := internalClient.GetSecret("secrets/internal/db")
          fmt.Printf("Internal secret: %v\n", secret1)

          // Route 2: External operations
          externalClient := createClientWithHint("external")
          secret2, _ := externalClient.GetSecret("secrets/public/api-key")
          fmt.Printf("External secret: %v\n", secret2)

          // Route 3: Admin operations (if admin SVID available)
          adminClient := createClientWithHint("admin")
          secret3, _ := adminClient.GetSecret("secrets/admin/root-creds")
          fmt.Printf("Admin secret: %v\n", secret3)
      }

      func createClientWithHint(hint string) *SpikeClient {
          // Create X509Source with custom SVID picker
          source, _ := workloadapi.NewX509Source(
              context.Background(),
              workloadapi.WithDefaultX509SVIDPicker(func(svids []*x509svid.SVID) *x509svid.SVID {
                  for _, s := range svids {
                      if s.Hint == hint {
                          log.Printf("Selected SVID %s for hint %q", s.ID, hint)
                          return s
                      }
                  }
                  log.Printf("No SVID found for hint %q, using default", hint)
                  return svids[0]
              }),
          )

          return &SpikeClient{source: source}
      }
      ```

      SPIRE REGISTRATION (spire-setup.sh):
      ```bash
      #!/bin/bash

      # Register workload with THREE different SPIFFE IDs
      # Same selectors (same workload), different identities

      # Internal identity
      spire-server entry create \
        -parentID spiffe://example.org/spire/agent/demo \
        -spiffeID spiffe://example.org/demo/app/internal \
        -selector unix:uid:1000 \
        -hint internal

      # External identity
      spire-server entry create \
        -parentID spiffe://example.org/spire/agent/demo \
        -spiffeID spiffe://example.org/demo/app/external \
        -selector unix:uid:1000 \
        -hint external

      # Admin identity (optional - shows privileged operations)
      spire-server entry create \
        -parentID spiffe://example.org/spire/agent/demo \
        -spiffeID spiffe://example.org/demo/app/admin \
        -selector unix:uid:1000 \
        -hint admin
      ```

      SPIKE POLICIES (spike-policies.sh):
      ```bash
      #!/bin/bash

      # Policy 1: Internal SVID can read internal secrets
      spike policy create internal-access \
        --spiffe-id-pattern "spiffe://example\\.org/demo/app/internal" \
        --path-pattern "secrets/internal/.*" \
        --permissions read

      # Policy 2: External SVID can read public secrets
      spike policy create external-access \
        --spiffe-id-pattern "spiffe://example\\.org/demo/app/external" \
        --path-pattern "secrets/public/.*" \
        --permissions read

      # Policy 3: Admin SVID has full access
      spike policy create admin-access \
        --spiffe-id-pattern "spiffe://example\\.org/demo/app/admin" \
        --path-pattern "secrets/admin/.*" \
        --permissions read,write,delete
      ```

      DEMONSTRATION FLOW:

      1. Setup:
         - Deploy SPIRE, SPIKE, demo workload
         - Register three SPIFFE IDs with same selectors
         - Create three SPIKE policies

      2. Run workload:
         - Workload receives 3 SVIDs from SPIRE
         - Logs all received SVIDs and hints
         - Makes three separate connections to SPIKE Nexus:
           a. With "internal" SVID â†’ reads secrets/internal/db
           b. With "external" SVID â†’ reads secrets/public/api-key
           c. With "admin" SVID â†’ reads secrets/admin/root-creds

      3. Observe SPIKE logs:
         - Show each connection authenticated with different SPIFFE ID
         - Show policy enforcement based on presented identity
         - Demonstrate that same workload gets different access levels

      4. Security demonstrations:
         - Try accessing admin secrets with internal SVID â†’ 403 Forbidden
         - Try accessing internal secrets with external SVID â†’ 403 Forbidden
         - Show that policies are enforced per-identity

      README.md SECTIONS:

      1. Overview
         - What this demo shows
         - Why multiple SVIDs exist
         - When to use this pattern

      2. SPIFFE Concepts
         - X.509-SVID structure
         - Workload API and multiple identities
         - Hint field purpose
         - SVID selection in go-spiffe

      3. SPIKE Trust Model
         - SPIKE trusts SPIRE for identity issuance
         - SPIKE enforces policies based on presented SVID
         - Workload chooses which SVID to present
         - Separation of concerns (SPIRE vs SPIKE)

      4. Real-World Use Cases
         - Service mesh internal/external routing
         - Multi-tenant applications
         - Privilege separation within single process
         - Gateway/proxy scenarios

      5. Best Practices
         - âœ… DO: Use different selectors for different privilege levels
         - âŒ DON'T: Mix low/high privilege SVIDs on same selectors
         - âœ… DO: Use hints semantically ("internal", "external")
         - âœ… DO: Document why workload needs multiple identities
         - âŒ DON'T: Use multiple SVIDs as a workaround for bad design

      6. Troubleshooting
         - How to verify SVIDs received by workload
         - How to check which SVID is being used
         - Common misconfiguration scenarios
         - SPIKE policy debugging

      EDUCATIONAL VALUE:

      This demo teaches:
      - Multiple SVIDs are a feature, not a bug
      - Hint field usage in practice
      - go-spiffe WithDefaultX509SVIDPicker
      - SPIKE/SPIRE trust boundaries
      - Proper SPIRE selector configuration
      - Policy design for multi-identity workloads

      BONUS: Video Walkthrough
      Create a 5-10 minute video showing:
      1. Setup and configuration
      2. Running the demo
      3. Explaining what's happening
      4. Security model walkthrough
      5. Common pitfalls

      TESTING:
      - Verify all three identities work correctly
      - Test policy enforcement (positive and negative cases)
      - Verify hint-based selection
      - Test with missing hints
      - Test default SVID selection

      RELATED:
      - Complements documentation issue about multiple SVIDs
      - Provides hands-on learning
      - References SPIFFE/SPIRE standards
      - Shows SPIKE's correct behavior
    </issue>
  </examples>
</stuff>
